using LinearAlgebra
using SpecialFunctions

mutable struct Node

    value::Int64 # value of the link leading to node

    parent::Int64
    generation::Int64
    cylinder::Int64
    child_vals::Vector{Int64}
    child_locs::Vector{Int64}

    cn::Int64 # the count accumulated at each node n
    Pr::Float64 # the probability of the sequence leading to the node
    Pr_c::Vector{Float64} # the node-child transition probability

    morph_vals::Vector{Int64}
    morph_Pr_c::Vector{Float64}
    morph_children::Vector{Int64}
    morph_dist::Vector{Float64}
    morph_branches::Int64

    state::Int64

    function Node(value::Int64,
                parent::Int64, generation::Int64, cylinder::Int64,
                child_vals::Vector{Int64}, child_locs::Vector{Int64},
                cn::Int64, Pr::Float64, Pr_c::Vector{Float64},
                morph_vals::Vector{Int64}, morph_Pr_c::Vector{Float64},
                morph_children::Vector{Int64}, morph_dist::Vector{Float64},
                morph_branches::Int64,
                state::Int64)

        new(value,
            parent, generation, cylinder,
            child_vals, child_locs,
            cn, Pr, Pr_c,
            morph_vals, morph_Pr_c,
            morph_children, morph_dist,
            morph_branches,
            state)
    end

    function Node(value, parent)

        child_vals = Array{Int64, 1}(undef, 0)
        child_locs = Array{Int64, 1}(undef, 0)
        cn = 0
        Pr = 0.0
        Pr_c = Array{Float64, 1}(undef, 0)
        generation = 0
        cylinder = 0

        morph_vals = Array{Int64, 1}(undef, 0)
        morph_Pr_c = Array{Float64, 1}(undef, 0)
        morph_children = Array{Int64, 1}(undef, 0)
        morph_dist = Array{Float64, 1}(undef, 0)
        morph_branches = 0

        state = 0

        Node(value,
            parent, generation, cylinder,
            child_vals, child_locs,
            cn, Pr, Pr_c,
            morph_vals, morph_Pr_c,
            morph_children, morph_dist,
            morph_branches,
            state)
    end
end

mutable struct Parse_Tree

    #= Theory
        A window of width D is advanced through s one symbol at a time.
        Therefore, the window lags behind the process D number of time steps.

        D is an approximation parameter. The longer it is, the more correlation,
        structure, and so on, is captured by the tree. Since, s is of finite length
        the tree depth cannot be indefinitely extended. Thus, to obtain good estimates
        of the subsequences generated by the process and of the tree structure the
        tree depth mustbe set at some optimal value. That value in turn depends on the
        amount of available data.

        A tree T = {n,l} consists of nodes n = {ni} and directed, labeled links
        l = {li} connecting them in a hierarchical structure with no closed paths.
        The links are labeled by the measurement symbols.

        An D-level subtree T(n,D) is a tree that starts at node n and contains all
        nodes below n that can be reached within D links. To construct a tree from a
        measurement sequence we simply parse the latter for all length D sequences
        and from this construct the tree with links up to level D that are labeled
        with individual symbols up to that time. We refer to length D subsequences
        as D-cylinders. Hence an D level tree has a length D path corresponding to
        each distinct observed D-cylinder. That is, we refer to length D subsequences 
        as D-cylinders. Hence an D level tree has a length D path corresponding to each 
        distinct name for that bundle of underlying process's orbits each of which 
        visited the sequence of measurement partition elements indexed by the D-cylinder. 
        The basic assumption in building a tree is that symbol sequences observed at 
        different times in the data stream approximate the same process state. Nonstationary 
        processes are examples for which this assumption fails.

        The tree is built usin a sliding window to move through the data stream. It 
        captures in this way the distinct sequences and summarises their occurrence at 
        different points in the data stream via a count or probability. That is, we refer to length D subsequences 
        as D-cylinders. Hence an D level tree has a length D path corresponding to each 
        distinct name for that bundle of underlying process's orbits each of which 
        visited the sequence of measurement partition elements indexed by the D-cylinder. 
        The basic assumption in building a tree is that symbol sequences observed at 
        different times in the data stream approximate the same process state. Nonstationary 
        processes are examples for which this assumption fails.

        The tree is built usin a sliding window to move through the data stream. It 
        captures in this way the distinct sequences and summarises their occurrence at 
        different points in the data stream via a count or probability.

        1. Get sequence of past D symbols
        2. Add probabilistic structure to the tree by recording for each node ni the
        number Ni(L) of occurrences of the associated L-cylinder relative to the
        total number N(L) observed.

        A partition is "generating" if sufficiently long binary sequences come from 
        arbitrarily small invervals of initial conditions. If the partition is generating 
        the resulting binary sequences capture the statistical properties of the map. 
        In other words, there is a one-to-one mapping between infinite binary sequences and 
        almost all points on the attractor. 

        This partition is generating and the resulting binary sequences completely
        capture the statistical properties of the Logistic map. In other words, there is
        a one-to-one mapping between infinite binary sequences and almost all points on
        the attractor.

        The generating partition is defined as a partition for which the topological 
        entropy achieves its supremum. 

        A tree machine is a subclass of finite-state automata with stochastic emission 
        probabilities and deterministeic state and deterministic state transitions, given 
        an emitted symbol. One follows recent symbols (the context) down the tree (deeper 
        corresponding to more ancient symbols) and upon matching a terminal node, defines 
        the state. The state emits independent symbols with a certain distribution. A 
        tree with all nodes at depth D is a D-order Markov chain. 

        The nontrivial issue, is estimating a suitable topolgy for the data, as that directly 
        addresses the complexity versus predictability issue, whether to use a shallow tree 
        whose nodes collect more data and hence are better local estimators or to use a deeper 
        tree because it is necessary to distinguish distinct states recognizable from the data. 
        There are 2^2^D topologies of binary trees with maximum depth no larger than D; for any but 
        the smallest D, choosing among them would appear to be prohibitively expensive. 
    =#

    Nodes::Vector{Node}
    N::Int64 # length of the total sequence
    D::Int64 # 1st Approximation Parameter - Depth of the tree (even number)
    L::Int64 # 2nd Approximation Parameter - length of future subsequences

    Cyls::Vector{Vector{Int64}} # empirical estimation of cylinders
    C_vols::Vector{Float64} # Cylinder volumes

    reps::Matrix{Float64} # Cylinder representatives
    H::Float64 # expected mean squared error
    m::Int64 # center point

    function Parse_Tree(Nodes::Vector{Node}, N::Int64, D::Int64, L::Int64,
        Cyls::Vector{Vector{Int64}}, C_vols::Vector{Float64},
        reps::Matrix{Float64}, H::Float64, m::Int64)
        new(Nodes, N, D, L, Cyls, C_vols, reps, H, m)
    end

    function Parse_Tree(N, D, m, H)

        Nodes = Array{Node, 1}(undef, 0)
        Nodes = [Nodes; Node(-1, 0)] # creating the root node
        Nodes[1].cn = Nodes[1].cn + 1
        Nodes[1].Pr = Nodes[1].cn/(N - D)

        L = Int(floor(D/2))

        Cyls = Vector{Vector{Int64}}(undef, 0)
        C_vols = Vector{Float64}(undef, 0)

        reps = Matrix{Float64}(undef, 0, 2)

        Parse_Tree(Nodes, N, D, L, Cyls, C_vols, reps, H, m)
    end
end

mutable struct ϵ_Machine

    #=
        Minimal hidden unifilar Markov Chain. A graph that is non-unifilar is a graph
        that has more than one link with the same label. Minimal deterministic automata

        An ϵ-machine is a stochastic automaton of the minimal computational power
        yielding a finite description of the data stream. Minimality is essential.
        It restricts the scope of properties detected in the ϵ-machine to be no
        larger than those possessed by the underlying physical system. We may assume
        at first that the data stream is governed by a stationary measure. That is,
        the probabilities of fixed length blocks of measurements exist and are
        time-translation invariant.

        An ϵ-machine reveals, in a very direct way, how the process stores information,
        and how that information is transformed by new inputs and by the passage of time.
        The algebra generated by the ϵ-machine is a semi-group with an identity element,
        i.e., it is a monoid. Due to this, ϵ-machines can be interpreted as capturing
        a process's generalized symmmetries. Any subgroups of an ϵ-machine's semi-group
        are, in fact, symmetries in the usual sense. 

        The fact that machine reconstruction uses observational data makes it similar
        to neural networks when finding a causal pattern in data, except that in the
        case of neural network, the discovered causal pattern is hidden from the
        modeler. In contrast, the reconstructed machine displays the computational
        structure of the process being modeled. Probabilistic and deterministic
        structure is discovered rather than imprinted onto the data. The machine
        representations captures pattern and regularities in the data in a way that
        reflects the causal structure of the process.

        ϵ Machine reconstruction deduces from the diversity of individual temporal
        patterns "generalized states", associated with the graph vertices, that are 
        optimal for forecasting. The topological ϵ-machines so reconstructed capture
        the essential computational aspects of the data stream by virtue of Occam's razor.

        The ϵ simply reminds us that what we are constructing is an approximation
        of the process's computational structure and depends on the measuring
        instrument's characteristics, such as its resolution. The procedure begins
        with a data stream and estimates the number of states and their transition
        structure and probabilities.

        The goal, is to reconstruct from a given physical process a computationally
        equivalent machine. The reconstruction technique, is quite general and applies
        directly to the modeling tasks for forecasting temporal or spatio-temporal
        data series. The resulting minimal machine's structure indicates the inherent
        information processing, i.e. transmission and computation, of the original
        physical process.

        Through a choice of symmetry that is appropriate for forecasting the data
        stream the conditional complexity C(D|S) can be estimated. The aim is to
        infer generalized "states" in the data stream that are optimal for
        forecasting. We will identify these staes with measurement sequences giving
        rise to the same set of possible future sequences. Using the temporal
        translation invariance guaranteed by stationarity, we identify these states
        using a sliding window that advances one measurement at a time through the
        sequence. This leads to a second step in the inference technique, the
        construction of a parse tree for the measurement sequence probability
        distribution. This is a coarse-grained representation of the underlying
        process's measure in orbit space. The state identification requirement
        then leads to an equivalece relation on the parse tree. The machine states
        correspond to the induced equivalence classes; the tate transitions, to the
        observed transitions in the tree between the classes.

        The overall procedure has two steps. The first is to identify the states and
        the second is to infer the transformation T. Once the states are found, the
        temporal evolution of the process, its (symbolic) dynamic, is given by a
        mapping from states to states T: S->S; that is, S(t+1) = T*S(t). T is the
        full probabilistic structure described by a set of input-alphabet labeled
        transition matrices.

        To infer the set of states we look for distinct subtrees of a given depth L.
        This is the length of future subsequences over which the morphs must agree.
        This step introduces a second approximation parameter, the morph depth L.
        The states should not only be topologically distinct, but also distinct in
        probability. That is, two states are in the same equivalence class, if their
        subtrees adhere to a similarity relation.

        The meaning of a measurement is the selected morph, the distribution associated 
        with the causal state to which the agent transitions, and the degree of meaning 
        is determined by the latter's probability. That is, it is the information flow 
        towards the new causal state.

        The meaning of a measurement is the selected morph, the distribution associated 
        with the causal state to which the agent transitions, and the degree of meaning 
        is determined by the latter's probability. That is, it is the information flow 
        towards the new causal state.

        The state-to-state transition structure is obtained by looking at how the
        morphs change into one another upon moving down the parse tree. Subtree
        equivalence means that the link structure is identical. Furthermore, two
        L-subtrees should be δ-similar and their corresponding links individually
        equally probable within some margin δ. The state-to-state transition
        probabilities are taken from the estimated node-to-node transition
        probabilities.

        As a whole, a machine is defined by a set of vertices, V = {v}, a set of
        transitions T, and an alphabet A. M = {V, E, A}. The graphical depiction of
        M is a labeled directed multigraph, or 1-digraph. They are related to the
        Shannon graphs of information theory, to Weiss's sofic systems in symbolic
        dynamics, to discrete finite automata in computation theory, and to
        regular languages in Chomsky's hierarchy. In particular it's a probabilistic
        version of these. Their topological structure can be described by a 1-digraph
        G = {V, E} that consists of vertices V = {vi} and directed edges E = {ei}
        connecting them, where each of the edges is labelled by a symbol s ϵ A.

        Some states may transition to dangling states, states that have no transitions
        to other states. Dangling states may indicate that the amount of data, N, is
        too low. However, determining the exact D and δ is tricky.

        A machine is a compact and very informative representation of the underlying
        source. In order to appreciate the properties that it captures, there are
        several statistics that can be computed for a given machine. Each of these is,
        naturally, a simplification in one way or another of the structure captured
        by the machine. For example, the machine can be reduced to a Markov process.

        ϵ-Machine reconstruction deduces from the diversity of individual patterns in
        data stream "generalised states", the morphs, associated with the graph vertices,
        that are optimal for forecasting. The topological ϵ-machines so reconstructed capture
        the estial computational aspects of the data stream by firtue of Occam's razor. A 
        morph is a set of cylinders.
    =#

    Ts::Vector{Matrix{Float64}} # The connectivity matrix / transition matrix - probability and symbol elements
    Tree::Parse_Tree
    δ::Float64 # Conditional Probability Margin - 3rd Approximation Parameter
    μ_m::Float64 # Sampling timestep / interval of the machine
    ϵ::Array{Float64, 1} # Instrument resolution
    x_range::Array{Float64, 2} # State space limits
    k::Int64 # Simulation step - number of coordinate points (duplicate in Tree)

    s::Array{Int64, 1} # measured symbols
    x::Array{Float64, 2} # input data
    x_m::Array{Float64, 2} # State space machine representation of symbols
    t_m::StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64} # Machine time
 
    C_states::Matrix{Int64} #Nodes, and macro checks
    start_state::Int64
    Dang_States::Vector{Int64}
    IG::Float64 # Graph Indeterminacy - the degree of ambiguity in transitions between graph vertices

    T::Matrix{Float64} #stochastic connection matrix - Markov process
    I::Matrix{Float64} #Information flow between states
    eigval::Vector{ComplexF64} #eigen values of T
    r_eigvecs::Matrix{ComplexF64} # right eigenvectors
    l_eigvecs::Matrix{ComplexF64} # left eigenvectors
    D::Matrix{ComplexF64} # diagonal matrix
    pv::Vector{Float64} # stationary state probability distribution
    
    α::StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}

    Tα::Vector{Matrix{Float64}} # paramatrized stochastic connection matrix of order α - perturbed matrix, αth Hadamar power of T - perturbed matrix, αth Hadamar power of T
    Iα::Vector{Matrix{Float64}} #Information flow between states
    eigval_α::Vector{Vector{ComplexF64}} #eigen values of T
    r_eigvecs_α::Vector{Matrix{ComplexF64}} # right eigenvectors
    l_eigvecs_α::Vector{Matrix{ComplexF64}} # left eigenvectors
    Dα::Vector{Matrix{ComplexF64}} # diagonal matrix
    pv_α::Vector{Vector{Float64}} # stationary state probability distribution

    Zα::Vector{Float64} # Partition function of order α
    Hα::Vector{Float64} # total Renyi entropy or "free information"
    hα::Vector{Float64} # Renyi specific entropy, i.e. entropy per measurement
    Cα::Vector{Float64} # α-order graph complexity
    Cαe::Vector{Float64} # α-order asymptotic transition edge complexity
    
    Cμ_t::Vector{Float64} # Time Series of Statistical Complexity
    h::Float64 # Topological Entropy
    hμ::Float64 # Entropy rate - growth rate of Shannon information in subsequences
    C::Float64 # State (recurrent) topological Complexity
    Cμ::Float64 # Statistical Complexity - measures amount of memory in the source
    Ce::Float64 # Transition topological Complexity
    Cμe::Float64 # Statistical Edge Complexity

    Hpe::Vector{Float64} # normalised permutation entropy 
    hpe::Vector{Float64} # normalised permutation entropy per symbol

    function ϵ_Machine(Ts::Vector{Matrix{Float64}}, Tree::Parse_Tree,
        δ::Float64, μ_m::Float64, ϵ::Array{Float64, 1}, x_range::Array{Float64, 2}, k::Int64,
        s::Array{Int64, 1}, x::Array{Float64, 2}, x_m::Array{Float64, 2}, t_m::StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64},
        C_states::Matrix{Int64}, start_state::Int64, Dang_States::Vector{Int64}, IG::Float64,
        T::Matrix{Float64}, I::Matrix{Float64}, eigval::Vector{ComplexF64},
        r_eigvecs::Matrix{ComplexF64}, l_eigvecs::Matrix{ComplexF64},
        D::Matrix{ComplexF64}, pv::Vector{Float64}, 
        α::StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64},
        Tα::Vector{Matrix{Float64}}, Iα::Vector{Matrix{Float64}}, 
        eigval_α::Vector{Vector{ComplexF64}}, r_eigvecs_α::Vector{Matrix{ComplexF64}}, 
        l_eigvecs_α::Vector{Matrix{ComplexF64}}, Dα::Vector{Matrix{ComplexF64}}, pv_α::Vector{Vector{Float64}},
        Zα::Vector{Float64}, Hα::Vector{Float64}, hα::Vector{Float64}, Cα::Vector{Float64}, Cαe::Vector{Float64}, 
        Cμ_t::Vector{Float64}, h::Float64, hμ::Float64, C::Float64, Cμ::Float64, Ce::Float64, Cμe::Float64,
        Hpe::Vector{Float64}, hpe::Vector{Float64})     

        new(Ts, Tree, δ, μ_m, ϵ, x_range, k,
        s, x, x_m, t_m,
        C_states, start_state, Dang_States, IG,
        T, I, eigval, 
        r_eigvecs, l_eigvecs, D, pv,  
        α,
        Tα, Iα, 
        eigval_α, r_eigvecs_α, l_eigvecs_α, 
        Dα, pv_α,
        Zα, Hα, hα, Cα, Cαe,
        Cμ_t, h, hμ, C, Cμ, Ce, Cμe,
        Hpe, hpe)
    end

    function ϵ_Machine(N, D, ϵ, x_range, μ_m, μ_s; δ = 0.05)

        # Stochastic transitions on a Multigraph
        Ts = Vector{Matrix{Float64}}(undef, 0)
        C_states = Array{Int64, 2}(undef, 0, 4)
        Tree = Parse_Tree(N, D, 0, 0.0)
        start_state = 0
        Dang_States = Array{Int64, 1}(undef, 0)
        IG = 0.0

        μ_m = round(μ_m/μ_s)*μ_s # correction to prevent overflow of vectors
        k = 0 # machine step
        t_final = N*μ_s - μ_s
        
        N = convert(Int64, Int(ceil(N*μ_s/μ_m)))

        dim = length(ϵ)
        s = Array{Int64, 1}(undef, N)
        s = fill!(s, 0)
        x = Array{Float64, 2}(undef, dim, N) # State space machine representation
        x_m = Array{Float64, 2}(undef, dim, N) # State space machine representation
        t_m = 0:μ_m:t_final # machine time

        # Markov process
        T = Array{Float64, 2}(undef, 0, 0)
        I = Array{Float64, 2}(undef, 0, 0)
        eigval = Array{ComplexF64, 1}(undef, 0)
        r_eigvecs = Array{ComplexF64, 2}(undef, 0, 0)
        l_eigvecs = Array{ComplexF64, 2}(undef, 0, 0)
        D = Array{ComplexF64, 2}(undef, 0, 0)
        pv = Array{Float64, 1}(undef, 0)

        Cμ_t = Array{Float64, 1}(undef, length(t_m))
        h = 0.0
        hμ = 0.0
        Cμ = 0.0
        Cμe = 0.0
        C = 0.0
        Ce = 0.0

        α = 0:0.1:1.0
        lα = length(α)
 
        Tα = Vector{Matrix{Float64}}(undef, lα)
        Iα = Vector{Matrix{Float64}}(undef, lα)
        eigval_α = Vector{Vector{ComplexF64}}(undef, lα)
        r_eigvecs_α = Vector{Matrix{ComplexF64}}(undef, lα)
        l_eigvecs_α = Vector{Matrix{ComplexF64}}(undef, lα)
        Dα = Vector{Matrix{ComplexF64}}(undef, lα)
        pv_α = Vector{Vector{Float64}}(undef, lα)

        Zα = Array{Float64, 1}(undef, lα)
        Zα = fill!(Zα, 0.0)
        Hα = Array{Float64, 1}(undef, lα)
        Hα = fill!(Hα, 0.0)
        hα = Array{Float64, 1}(undef, lα)
        hα = fill!(hα, 0.0)
        Cα = Array{Float64, 1}(undef, lα)
        Cα = fill!(Cα, 0.0)
        Cαe = Array{Float64, 1}(undef, lα)
        Cαe = fill!(Cαe, 0.0)

        Hpe = Array{Float64, 1}(undef, dim)
        Hpe = fill!(Hpe, 0.0)
        hpe = Array{Float64, 1}(undef, dim)
        hpe = fill!(hpe, 0.0)

        ϵ_Machine(Ts, Tree, δ, μ_m, ϵ, x_range, k,
        s, x, x_m, t_m,
        C_states, start_state, Dang_States, IG,
        T, I, eigval, 
        r_eigvecs, l_eigvecs, D, pv,  
        α,
        Tα, Iα,  
        eigval_α, r_eigvecs_α, l_eigvecs_α, 
        Dα, pv_α,
        Zα, Hα, hα, Cα, Cαe,
        Cμ_t, h, hμ, C, Cμ, Ce, Cμe,
        Hpe, hpe)
    end
end

function Partitioning(Deus::ϵ_Machine, x)

    #= Theory:
        Every instrument implements a very basic inherent coarse-graining. Typically,
        this would be the measurement sensors and transducers. A transducer is a
        machine that not only reads symbols from some input, taking the appropriate
        transitions, but also outputs a symbol determined by each transition taken.
        In the graphical representation the edges of a transducer are labeled with
        both input and output symbols. The latter may come from different alphabets.

        A partition is a finite collection of disjoint sets whose union is the state space.

        1. Simulate the measurement instrument
            a. m is the number of experimental probes
            b. An instrument that distinguishes the projected states to within a
            resolution ϵ partitions the measurement space into a set of cells.
            c. Each cell is the equivalence class of projected states projected states
            that are indistinguishable using the instrument.
            d. The cells have volume ϵ^m
            e. The (unknowable) exact states of the observed system are translated
            into a sequence of symbols via a measurement chanel.
            f. The instrument maps a sequence of states into a sequence of symbols.
            g. Define an alphabet of symbols, which have k~ϵ^-m partition elements,
            if the inputs are normalised
            h. Implement maximum and minimum limits

        The generating partition requirement necessarily determines the number of
        probes required by the instrument. The (unknowable) exact states of the
        observed system are translated into a sequence of symbols via a measurement
        channel. This process is described by a parametrized partition of the state
        space. A generating partition requires infinite sequences of cell indices 
        to be in a finite-to-one correspondence with the process's states. The notion 
        of generating partitions gives an operational definition of a good instrument 
        that is useful if the instrument can be altered. For an instrument with a
        generating partition Shannon's noiseless coding theorem says that the measurement 
        channel must have a capacity higher than the process's entropy rate. A theorem 
        of Kolmogorov says that the entropy rate is maximised for a given process if 
        partition is generating.

        The notion of generating partition in symbolic dynamics is based on the "splitting" 
        of the phase space in terms of measureable sets. 

        The theory of symbolic dynamics is a tool for the investigation of discrete time 
        dynamical systems. The main idea is that of a partitio, that is a finite collection 
        of disjoint subsets whose union is the state space. By identifying each point 
        in the state space with a unique symbol, we have a sequence of symbols that correspond 
        to each trajectory of the original system - the sequence is produced as the evolving state 
        visits different regions of the partition. This idea is at its most powerful 
        when the partition is chosen to be generating, that is, when the assingment of symbol 
        sequences to trajectories is unique, up to a set of measure zero. 

        When the partition is a generating partition, the original dyanmics and the 
        symbol dynamics are conjugate, (i.e. acting or operating as if joined).

        A partition which always yields a reduction in uncertainty as to the exact microstate 
        given increasing coarse-grained measurements, is known as a generating partition.
    =#

    dims = length(x) # number of dimensions

    s = 0 # mapped symbol
    A = 1 # number of symbols in the alphabet

    for d in 1:dims

        norm = (Deus.x_range[d, 1] - Deus.x_range[d, 2])

        if x[d] >= Deus.x_range[d, 1] #maximum not included
            x[d] = Deus.x_range[d, 1] - 0.1*Deus.ϵ[d]*norm
        end
        if x[d] < Deus.x_range[d, 2]
            x[d] = Deus.x_range[d, 2]
        end

        x[d] = x[d] - Deus.x_range[d, 2]
        s = s + Int(floor(x[d]/(Deus.ϵ[d]*norm))*A)

        A = Int(A*ceil(1/Deus.ϵ[d]))
    end

    return s, A
end

function Dimensioning(Deus::ϵ_Machine, s)

    #=
        This function provides the inverse of the Partitioning function. A symbol
        is mapped back to the state space.
    =#

    m = length(Deus.ϵ) # number of dimensions

    x = Array{Float64, 1}(undef, m) # State space position
    k = [1; Int.(ceil.(1 ./ Deus.ϵ[:]))]

    for d in 1:m
        norm = (Deus.x_range[d, 1] - Deus.x_range[d, 2])
        x[d] = (norm/k[d+1])*floor((s/prod(k[1:d]))%(k[d+1])) + Deus.x_range[d, 2] + norm*Deus.ϵ[d]/2
    end

    return x
end

function Sampling(Deus::ϵ_Machine, μ_s)

    for i in axes(Deus.x, 2)

        if i%round(Deus.μ_m/μ_s) == 0

            Deus.k = Deus.k + 1
  
            #Deus.s[Deus.k], _ = Partitioning(Deus, Deus.x[:, i])
            #Deus.x_m[:, Deus.k] = Dimensioning(Deus, Deus.s[Deus.k])

            rnd = rand()
            if rnd >= 2/3
                Deus.s[Deus.k] = 2
            elseif rnd >= 1/3
                Deus.s[Deus.k] = 1
            elseif rnd >= 0
                Deus.s[Deus.k] = 0
            end
        end
    end

    return nothing
end

function add_Nodes(Deus::ϵ_Machine, s, mi; parent = 1)

    d = length(s) # Depth of the remaining sub-sequence

    Tree = Deus.Tree

    if length(Tree.Nodes[parent].child_vals) == 0

        new_node = add_Node(Tree, parent, s[1])

        Tree.Nodes[parent].Pr_c = [Tree.Nodes[parent].Pr_c; Tree.Nodes[new_node].cn/Tree.Nodes[parent].cn]

        if d > 1
            add_Nodes(Deus, s[2:end], mi, parent = new_node)
        else

            Tree.Cyls = push!(Tree.Cyls, Vector{Int64}(undef, 1))
            Tree.Nodes[new_node].cylinder = length(Tree.Cyls)
            Tree.Cyls[end] = [mi]
        end
    else

        parent_loc = findfirst(x->x == s[1], Tree.Nodes[parent].child_vals)

        Tree.Nodes[parent].cn = Tree.Nodes[parent].cn + 1
        Tree.Nodes[parent].Pr = Tree.Nodes[parent].cn/(Tree.N - Tree.D)

        if parent_loc !== nothing

            new_parent = Int(Tree.Nodes[parent].child_locs[parent_loc])

            if d > 1
                add_Nodes(Deus, s[2:end], mi, parent = new_parent)
            else

                Tree.Nodes[new_parent].cn = Tree.Nodes[new_parent].cn + 1
                Tree.Nodes[new_parent].Pr = Tree.Nodes[new_parent].cn/(Tree.N - Tree.D)

                Tree.Cyls[Tree.Nodes[new_parent].cylinder] = [Tree.Cyls[Tree.Nodes[new_parent].cylinder]; mi]
            end
        else

            new_node = add_Node(Tree, parent, s[1])

            Tree.Nodes[parent].Pr_c = [Tree.Nodes[parent].Pr_c; 0]

            if d > 1
                add_Nodes(Deus, s[2:end], mi, parent = new_node)
            else

                Tree.Cyls = push!(Tree.Cyls, Vector{Int64}(undef, 1))
                Tree.Nodes[new_node].cylinder = length(Tree.Cyls)
                Tree.Cyls[end]= [mi]
            end
        end

        children = Tree.Nodes[parent].child_locs

        for i in eachindex(children)
            Tree.Nodes[parent].Pr_c[i] = Tree.Nodes[children[i]].cn./Tree.Nodes[parent].cn
        end
    end

    return nothing
end

function add_Node(Tree::Parse_Tree, parent, s)

    new_node = length(Tree.Nodes) + 1
    Tree.Nodes[parent].child_vals = [Tree.Nodes[parent].child_vals; s[1]]
    Tree.Nodes[parent].child_locs = [Tree.Nodes[parent].child_locs; new_node]
    Tree.Nodes = [Tree.Nodes; Node(s[1], parent)]

    Tree.Nodes[new_node].cn = Tree.Nodes[new_node].cn + 1
    Tree.Nodes[new_node].Pr = Tree.Nodes[new_node].cn/(Tree.N - Tree.D)

    Tree.Nodes[new_node].generation = Tree.Nodes[parent].generation + 1

    return new_node
end

function Cylinder_Volumes(Deus::ϵ_Machine, m)

    num_cyl = length(Deus.Tree.Cyls)

    lib = QHull.Library()

    max_C_vol = 0.0

    for i in 1:num_cyl

        if size(Deus.Tree.Cyls[i], 1) == 1 # this is a point

            Deus.Tree.C_vols = [Deus.Tree.C_vols; 0.0]
    
        elseif size(Deus.x, 1) == 1 # these are scalar values

            Deus.Tree.C_vols = [Deus.Tree.C_vols; (maximum(Cyls[i]) - minimum(Cyls[i]))]

        elseif size(Deus.Tree.Cyls[i], 1) == 2 # these is a line

            Deus.Tree.C_vols = [Deus.Tree.C_vols; 
            norm(Deus.x[:, m + Deus.Tree.Cyls[i][1]] .- Deus.x[:, m + Deus.Tree.Cyls[i][2]])]

        elseif size(Deus.Tree.Cyls[i], 1) > 2

            p = polyhedron(vrep(Deus.x[:, m .+ Deus.Tree.Cyls[i]]) , lib)
            removevredundancy!(p)
            Deus.Tree.C_vols = [Deus.Tree.C_vols; p.volume]
        end


        if Deus.Tree.C_vols[end] > max_C_vol
            max_C_vol = Deus.Tree.C_vols[end]
        end
    end

    return max_C_vol
end

function Symbolic_Shadowing(Deus::ϵ_Machine)

    max_iter = 50
    D_max = Deus.Tree.D

    D = 4 # initial string/cylinder length
    m_start = 2

    iter = 1
        
    m_opt = m_start
    H_min = 0.0

    count = Vector{Int64}(undef, 3)

    while iter <= max_iter
  
        Deus.Tree = Parse_Tree(N, D, m_opt, H_min) # initialising an empty tree

        for i in 1:Deus.k - Deus.Tree.D + 1
            add_Nodes(Deus, Deus.s[i : (i + Deus.Tree.D - 1)], i)
        end

        #= println("iter = ", iter)
        println("D = ", D) =#

        #_______________________________________________________________________________

        num_cyl = length(Deus.Tree.Cyls)
        Deus.Tree.reps = Matrix{Float64}(undef, num_cyl, length(Deus.x[:, 1]))

        s_new = Array{Int64, 1}(undef, Deus.k)
        reps_new = Matrix{Float64}(undef, num_cyl, length(Deus.x[:, 1]))

        for m in m_start:2#Deus.Tree.L

            #_______________________________________________________________________________
            # Representatives and Calculating the Expected/mean Squared Error

            H = 0.0

            for i in 1:num_cyl

                reps_new[i, :] = sum(Deus.x[:, m .+ Deus.Tree.Cyls[i]], dims = 2)/length(Deus.Tree.Cyls[i])

                for j in eachindex(Deus.Tree.Cyls[i])

                    H = H + (norm(Deus.x[:, m + Deus.Tree.Cyls[i][j]] .- reps_new[i, :]))^2
                end
            end

            H = H/Deus.k

            # Saving the best results
            if (m == m_start && iter == 1) || H < Deus.Tree.H

                Deus.Tree.H = H
                Deus.Tree.m = m
                Deus.Tree.reps = reps_new
                m_opt = m
                H_min = H

            elseif m == Deus.Tree.m

                Deus.Tree.H = H
                Deus.Tree.reps = reps_new
                H_min = H
            end
        end

        #= println("Mean squared error = ", Deus.Tree.H)
        println("Center point = ", Deus.Tree.m) =#

        #_______________________________________________________________________________
        # Now that the optimal parameters have been found, assign the new symbols

        count[1:2] = count[2:3]
        count[end] = 0

        for i in 1:Deus.k

            _, min_d = findmin(norm.(eachrow(transpose(Deus.x[:, i]) .- Deus.Tree.reps[:, :])))

            if Deus.s[i] != Deus.s[Deus.Tree.Cyls[min_d][1] + Deus.Tree.m]
                count[end] = count[end] + 1 #the new symbol sequence is different from the first
            end

            s_new[i] = Deus.s[Deus.Tree.Cyls[min_d][1] + Deus.Tree.m]
        end

        if count[end] != 0
            Deus.s = s_new
        end

        #= println("count = ", count[end])
        println() =#

        if  D != D_max && (count[end] == 0 || iter == max_iter)

            D = D + 1
            iter = 0

        elseif count[end] == 0 || all(count[1] .== count) # if it starts cycling
            break
        end

        iter += 1
    end
    #----------------------------------------------------------

    println("\n____________________________________________________")
    println("Symbolic Shadowing\n")
    println("Mean squared error = ", round(Deus.Tree.H, digits = 3))
    println("Replacement count = ", round(count[end], digits = 3))

    return nothing
end

function Tree_Isomorphism(Deus::ϵ_Machine; Ancest::Vector{Int64} = [1], d = 0)

    # Creating a unique topological isomorphism encoding

    all_vals = Array{Int64, 1}(undef, 0)
    all_Pr_c = Array{Float64, 1}(undef, 0)
    all_num_childs = Array{Int64, 1}(undef, 0)
    all_branches = Array{Int64, 1}(undef, 0)
    all_dist = Array{Float64, 1}(undef, 0)
    dist = Array{Float64, 1}(undef, 0)

    d = d + 1

    for i in eachindex(Ancest)

        child_vals = Deus.Tree.Nodes[Ancest[i]].child_vals
        num_children = length(child_vals)

        perm = sortperm(Deus.Tree.Nodes[Ancest[i]].child_vals)

        sorted_locs = Deus.Tree.Nodes[Ancest[i]].child_locs[perm]
        sorted_vals = Deus.Tree.Nodes[Ancest[i]].child_vals[perm]
        sorted_Pr_c = Deus.Tree.Nodes[Ancest[i]].Pr_c[perm]

        if num_children != 0 && d < Deus.Tree.L && (length(Deus.Tree.Nodes[Ancest[i]].morph_vals) == 0)

            child_morph_vals, child_morph_Pr_c, num_child_morph, child_morph_dist, child_branches =
            Tree_Isomorphism(Deus, Ancest = sorted_locs, d = d)

            ancest_branches = sum(child_branches)
            all_vals = [all_vals; sorted_vals; child_morph_vals]
            all_Pr_c = [all_Pr_c; sorted_Pr_c; child_morph_Pr_c]
            all_branches = [all_branches; ancest_branches]
            all_num_childs = [all_num_childs; num_children; num_child_morph]
            dist = empty!(dist)

            sum_child = 1
            if ancest_branches > 0
                for j in 1:num_children  

                    dist = [dist; sorted_Pr_c[j].*child_morph_dist[sum_child:sum_child + child_branches[j] - 1]]
                    sum_child = sum_child + child_branches[j]
                end
            end

            all_dist = [all_dist; dist]

            if d == 1

                Tree_Isomorphism(Deus, Ancest = sorted_locs)

                Deus.Tree.Nodes[Ancest[i]].morph_vals = [Deus.Tree.Nodes[Ancest[i]].morph_vals; sorted_vals; child_morph_vals]
                Deus.Tree.Nodes[Ancest[i]].morph_Pr_c = [Deus.Tree.Nodes[Ancest[i]].morph_Pr_c; sorted_Pr_c; child_morph_Pr_c]
                Deus.Tree.Nodes[Ancest[i]].morph_children = [Deus.Tree.Nodes[Ancest[i]].morph_children; num_children; num_child_morph]
                Deus.Tree.Nodes[Ancest[i]].morph_branches = ancest_branches
                Deus.Tree.Nodes[Ancest[i]].morph_dist = [Deus.Tree.Nodes[Ancest[i]].morph_dist; dist]

                if Deus.Tree.Nodes[Ancest[i]].generation - Deus.Tree.D + Deus.Tree.L <= 0

                    Reconstruction(Deus, Ancest[i])
                end
            end
        else

            all_vals = [all_vals; sorted_vals]
            all_Pr_c = [all_Pr_c; sorted_Pr_c]
            all_num_childs = [all_num_childs; num_children]
            all_dist = [all_dist; sorted_Pr_c]
            all_branches = [all_branches; num_children]
        end
    end

    return all_vals, all_Pr_c, all_num_childs, all_dist, all_branches
end

function Reconstruction(Deus::ϵ_Machine, Node)

    num_states = length(Deus.Ts)
    l_vals = length(Deus.Tree.Nodes[Node].morph_vals)
    num_children = length(Deus.Tree.Nodes[Node].child_vals)
    parent = Deus.Tree.Nodes[Node].parent
    flag = 0

    if num_states == 0

        Deus.C_states = cat([Node; parent; l_vals; num_children], dims = 2)
        Deus.Tree.Nodes[Node].state = 1 # this will be a dangling state
        Deus.Ts = push!(Deus.Ts, Matrix{Float64}(undef, 3, 0))
        Deus.Ts[1] = cat([1; -1; 1.0], dims = 2)
        Deus.Dang_States = [Deus.Dang_States; Deus.Tree.Nodes[Node].state]
    else

        for n in 1:num_states

            if l_vals == Deus.C_states[3, n] && num_children == Deus.C_states[4, n]

                state_vals = Deus.Tree.Nodes[Deus.C_states[1, n]].morph_vals

                node_vals = Deus.Tree.Nodes[Node].morph_vals

                if node_vals == state_vals

                    state_dist = Deus.Tree.Nodes[Deus.C_states[1, n]].morph_dist
                    node_dist = Deus.Tree.Nodes[Node].morph_dist
                    #Pr_d = maximum(abs.(node_dist .- state_dist))

                    #state_dist = Deus.Tree.Nodes[Deus.C_states[1, n]].Pr_c
                    #node_dist = Deus.Tree.Nodes[Node].Pr_c
                    #Pr_d = maximum(abs.(node_dist .- state_dist))

                    state_cn = Deus.Tree.Nodes[Deus.C_states[1, n]].cn
                    node_cn = Deus.Tree.Nodes[Node].cn

                    if  χ2_test(state_dist, node_dist, state_cn, node_cn; α = Deus.δ) # Pr_d <= Deus.δ # 

                        flag = 1 # don't create new causal state
                        Deus.Tree.Nodes[Node].state = Deus.Tree.Nodes[Deus.C_states[1, n]].state

                        #taking the average of the two morphs
                        cn_state = Deus.Tree.Nodes[Deus.C_states[1, n]].cn
                        cn_comp = Deus.Tree.Nodes[Node].cn
                        cn_total = cn_state + cn_comp

                        new_dist = cn_state*(Deus.Tree.Nodes[Deus.C_states[1, n]].morph_dist) .+ cn_comp*(Deus.Tree.Nodes[Node].morph_dist)
                        Deus.Tree.Nodes[Deus.C_states[1, n]].morph_dist = new_dist./cn_total
                        Deus.Tree.Nodes[Deus.C_states[1, n]].cn = cn_total

                        # making sure start state appears in table
                        if Node == 1 
                            
                            Deus.C_states[1, n] = 1
                            Deus.C_states[2, n] = 0
                            Deus.Tree.Nodes[1].morph_dist = Deus.Tree.Nodes[Deus.C_states[1, n]].morph_dist
                            Deus.Tree.Nodes[1].morph_Pr_c = Deus.Tree.Nodes[Deus.C_states[1, n]].morph_Pr_c
                            Deus.Tree.Nodes[1].Pr_c = Deus.Tree.Nodes[Deus.C_states[1, n]].Pr_c
                            Deus.start_state = Deus.Tree.Nodes[1].state
                        end

                        # if compare state is dangling - overwrite and rewire
                        # don't overwrite if state is low gen or already overwritten

                        state_test = Deus.Tree.Nodes[Deus.C_states[1, n]].generation - Deus.Tree.D + Deus.Tree.L # state is at bottom

                        if state_test == 0 && Deus.Ts[Deus.Tree.Nodes[Node].state][2, 1] == -1
                            
                            Add_Transitions(Deus, Node, n, num_children, overwrite = 1)

                        elseif state_test <= 0 && 1 == 2

                            # check if the new compare state has transitions to other states other than those of the current causal state
                            # if yes, then add those transitions, because they contribute to the graph indeterminacy

                            trans_states = Deus.Ts[n][1, :]
                            child_locs = Deus.Tree.Nodes[Node].child_locs

                            for c in eachindex(child_locs)

                                comp_state = Deus.Tree.Nodes[child_locs[c]].state

                                if comp_state != 0
                                    
                                    comp_val = Deus.Tree.Nodes[child_locs[c]].value
                                    found_states = findall(x->x == comp_state, trans_states)

                                    if isempty(found_states)

                                        Add_Transitions(Deus, Node, n, num_children)
                                        break

                                    elseif findfirst(x->x == comp_val, Deus.Ts[n][2, found_states]) === nothing

                                        Add_Transitions(Deus, Node, n, num_children)
                                        break
                                    end
                                end
                            end
                        end

                        break
                    end
                end
            end
        end

        if flag == 0

            Deus.C_states = cat(Deus.C_states, [Node; parent; l_vals; num_children], dims = 2)
            state_from = num_states + 1
            Deus.Tree.Nodes[Node].state = state_from
            Deus.Ts = push!(Deus.Ts, Matrix{Float64}(undef, 3, 0))

            if Node == 1
                Deus.start_state = Deus.Tree.Nodes[Node].state
            end

            Add_Transitions(Deus, Node, state_from, num_children)
        end
    end

    return nothing
end

function Add_Transitions(Deus::ϵ_Machine, Node, state_from, num_children; overwrite = 0)

    state_test = Deus.Tree.Nodes[Node].generation - Deus.Tree.D + Deus.Tree.L

    if state_test < 0

        if overwrite == 1
            Deus.Ts[state_from] = Matrix{Float64}(undef, 3, 0)
            Deus.Dang_States = deleteat!(Deus.Dang_States, findfirst(==(state_from), Deus.Dang_States))
        end

        child_locs = Deus.Tree.Nodes[Node].child_locs

        for n in 1:num_children

            state_to = Deus.Tree.Nodes[child_locs[n]].state
            symbol = Deus.Tree.Nodes[child_locs[n]].value
            state_Pr_c = Deus.Tree.Nodes[Node].Pr_c[n]

            Deus.Ts[state_from] = cat(Deus.Ts[state_from], [state_to; symbol; state_Pr_c], dims = 2)

        end

    elseif state_test == 0 && overwrite == 0# dangling state

        Deus.Ts[state_from] = cat(Deus.Ts[state_from], [state_from; -1; 1.0], dims = 2)
        Deus.Dang_States = [Deus.Dang_States; Deus.Tree.Nodes[Node].state]
    end

    return nothing
end

function Parametric_Statistical_Mechanics(Deus::ϵ_Machine)

    # parametrized stochastic connection matrix
    Deus.IG = 0.0 # do this if other stat mech was run

    num_edges = 0
    num_states = length(Deus.Ts)
    T = Array{Float64, 2}(undef, num_states, num_states)
    T = fill!(T, 0.0)

    id = length(Deus.α)
    Deus.Tα[id] = cat(T, dims = 2)

    for v in 1:num_states

        if Deus.Ts[v][2, 1] == -1

            Surgeon(Deus, v)
            num_states = length(Deus.Ts)
        end
    end

    num_states = length(Deus.Ts)

    for v in 1:num_states

        num_trans = length(Deus.Ts[v][1,:])
        num_edges = num_edges + num_trans

        trans = unique(Deus.Ts[v][1:2, :], dims = 2)
        num_trans = length(Deus.Ts[v][1,:])

        #normalise probabilities - only necessary if IG does not vanish
        pt = sum(Deus.Ts[v][3, :])  
        if pt != 1
            Deus.Ts[v][3, :] =  Deus.Ts[v][3, :]./pt
        end  
        
        for vd in axes(trans, 2)

            vloc = findall(i -> all(j -> trans[:, vd][j] == Deus.Ts[v][1:2, :][j,i], 1:2), 1:num_trans)

            pt = sum(Deus.Ts[v][3, vloc])

            state_to = Int(Deus.Ts[v][1, vloc[1]])
            Deus.Tα[id][v, state_to] = Deus.Tα[id][v, state_to] + pt
        end

        #Deus.Tα[id][v, :] = Deus.Tα[id][v, :]./sum(Deus.Tα[id][v, :])
    end

    for (id, α) in enumerate(Deus.α)

        if α == 0
            Deus.Tα[id] = Float64.(Deus.Tα[end] .!= 0) # the connectivity / adjacency matrix of T
        else
            Deus.Tα[id] = Deus.Tα[end].^α
        end

        Deus.Iα[id] = -log.(2, Deus.Tα[id]) # Information flow between nodes
        Deus.eigval_α[id] = eigvals(Deus.Tα[id]) # The eigenvalues for left and right eigenvectors are equal
        Deus.l_eigvecs_α[id] = eigvecs(transpose(Deus.Tα[id])) # the left eigenvectors are the right eigenvectors of the transpose
        Deus.r_eigvecs_α[id] = eigvecs(Deus.Tα[id])
        Deus.Dα[id] = Diagonal(Deus.eigval_α[id])

        _, index = findmax(real.(Deus.eigval_α[id]))

        if real(Deus.eigval_α[id][index]) >= 0.99

            Deus.pv_α[id] = abs.(Deus.l_eigvecs_α[id][1:end, index])/abs(sum(Deus.l_eigvecs_α[id][1:end, index]))

            for v in 1:num_states

                if Deus.pv_α[id][v] != 0

                    psn = (Deus.pv_α[id][v]).*(Deus.Tree.Nodes[Deus.C_states[1, v]].morph_dist)
                    sum_psn = sum(psn.^α)

                    Deus.Zα[id] = Deus.Zα[id] + sum(psn.^α) #sum(exp.(α.*log.(psn)))

                        if α == 1
                            
                            sum_psn = sum(psn.*log.(2, psn))

                        if isnan(sum_psn)
                            locs = findall( !=(0), psn) # remove all zeros
                            sum_psn = sum(psn[locs].*log.(psn[locs]))
                        end  

                            Deus.Hα[id] = Deus.Hα[id] - sum_psn

                        Deus.Cα[id] = Deus.Cα[id] - Deus.pv_α[id][v]*log(2, Deus.pv_α[id][v])  
                           
                    else
                        Deus.Cα[id] = Deus.Cα[id] + (Deus.pv_α[id][v])^α 
                        #Deus.Hα[id] = Deus.Hα[id] + sum(psn.^α) # as a check
                    end

                    syms = unique!(Deus.Ts[v][2, :])
                    psv = Array{Float64, 1}(undef, length(syms))

                    for s in eachindex(syms)
                        sloc = findall( ==(syms[s]), Deus.Ts[v][2, :])
                        psv[s] = sum(Deus.Ts[v][3, sloc])
                    end

                    trans = unique(Deus.Ts[v][1:2, :], dims = 2)
                    num_trans = length(Deus.Ts[v][1,:])

                        for vd in axes(trans, 2)

                        vloc = findall(i -> all(j -> trans[:, vd][j] == Deus.Ts[v][1:2, :][j,i], 1:2), 1:num_trans)

                        pt = sum(Deus.Ts[v][3, vloc])
                        pe = Deus.pv_α[id][v]*pt

                        if α == 1
    
                            Deus.hα[id] = Deus.hα[id] - pe*log(2, pt)
                            Deus.Cαe[id] = Deus.Cαe[id] - pe*log(2, pe)
                            
                            sym = Deus.Ts[v][2, vd]
                            sloc = findfirst( ==(sym), syms)

                            Deus.IG = Deus.IG - pe*log(2, pt/psv[sloc])
                        else
                            if α == 0
                                #Deus.pv_α[id][v] = 1/length(Deus.pv_α[id])
                            end
                            Deus.hα[id] = Deus.hα[id] + Deus.pv_α[id][v]*pt^α

                            Deus.Cαe[id] = Deus.Cαe[id] + pe^α
                        end
                    end
                end

            end

            if α != 1

                Deus.hα[id] = ((1 - α)^-1)*log(2, Deus.hα[id])
                Deus.Hα[id] = ((1 - α)^-1)*log(2, Deus.Zα[id])
                #Deus.Hα[id] = ((1 - α)^-1)*log(Deus.Hα[id]) # as a check

                Deus.Cαe[id] = ((1 - α)^-1)*log(2, Deus.Cαe[id])
                Deus.Cα[id] = ((1 - α)^-1)*log(2, Deus.Cα[id])
            end
        end    
    end

    println("\n____________________________________________________")
    println("Statistical Mechanics\n")
    println("Topological Entropy Rate, h = ", round(Deus.hα[1], digits = 3))
    println("Metric Entropy Rate, hμ = ", round(Deus.hα[end], digits = 3))
    println("Topolical Forecasting Entropy, C = ", round(Deus.Cα[1], digits = 3))
    println("Metric Forecasting Entropy, Cμ = ", round(Deus.Cα[end], digits = 3))
    println("Topolical Edge Complexity, Ce = ", round(Deus.Cαe[1], digits = 3))
    println("Metric Edge Complexity, Cμe = ", round(Deus.Cαe[end], digits = 3))
    println("Periods = ", round(2^(Deus.Hα[end]), digits = 3))

    return nothing
end

function Statistical_Mechanics(Deus::ϵ_Machine)

    #=
        Many of the important properties of stocahstic automate models are given concisely
        using a statistical mechanical formalism that describes the coarse-grained scaling
        of orbit space. 

        In a Markov chain we may distinguis h between two subsets of vertices. The first 
        Vt consists of those associated with transient states; the second Vr, consists 
        of recurrent states. 

        One useful reduction of an ϵ-Machine is to ask for its equivalent Markov
        process, which can be described by a stochastic connection matrix containing
        the state to state transition probabilities, uncondition by the measurement
        symbols. By construction, every state has an outgoing transition. The effect
        is losing the "computational" structure of the input data stream. All that
        is retained in T is the state transition structure and this is a Markov chain.
        The interesting fact is that Markov chains are proper subset of stochastic
        finite machines.

        Shannon entropy is a method to compute the uncertainty in an event. This can
        be used to compute the uncertainty in a stochastic process. However, it is
        easy to extend such that it can be used. Also when a stochastic process
        satisfies certain properties, for example if the stochastic process is a
        Markov process, it is straightforward to compute the entropy of the process.
        Recal that the entropy is the average number of bits to encode a single
        source symbol. If all of the random variables are i.i.d. each random variable
        emits symbols according to the same distribution. Therefore the output of
        each RV can be encoding using the same number of bits. If the entropy rate of
        a stochastic process is the average number of bits used to encode a source
        symbol it makes sense that for an i.i.d. stochastic process the entropy rate
        is equal to the entropy of its random variables.

        A stochastic process, also called a random process, is a set of random variables
        that model a non deterministic system. In other words the outcome of the system
        if not known on beforehand and the system can evolve in multiple ways. Put
        differently, a stochastic process is an indexed collection of random variables.
        There can be arbitrary dependence between each of the random variables. The
        stochastic process is characterised by the joint probabiliry mass function.
        A stochastic process is said to be stationary if the joint probability
        distribution of any subsequence of the sequence of random variables in
        invariant of shifts in time. The random variables in a stochastic process
        can have arbitrary dependence. However, if the random variables that a random
        variable can depend on are restricted to only its direct predecessor the
        stochastic process is called a Markov chain or a Markov process.

        A simple example of a stochastic process with dependence is one in which each 
        random variable depends on the one preceding it and is conditionally independent
        of all the other preceding random variables. Such a process is said to be Markov.
        The Markov chain is said to be time invariant if the conditional probability p(xn+1|xn)
        does not depend on n, i.e., for n = 1, 2, ... If it is possible to go with positive 
        probability from any state of the Markov chain to any other state in a finite number 
        of steps, then the Markov chain is said to be irreducible. 

        A distribution on the states such that the distribution at time n + 1 is the same as the
        distribution at time n is called a stationary distribution. The stationary distribution is 
        so called because if the initial state of a Markov chain is drawn according to a stationary
        distribution, then the Markov chain forms a stationary process.

        If the finite state Markov chain is irreducible and aperiodic, then the stationary distribution
        is unique, and from any starting distribution, the distribution of Xn tends to the stationary
        distribution as n -> ∞

        If we have a sequence of n random variables, a natural question to ask is "how does the
        entropy of the sequence grow with n. When the entropy rate exists in the limit, then
        we can define a related quantity for the entropy rate, which is a different notion 
        of entropy. The first is the per symbol entropy of the n random variables, and 
        the second is the conditional entropy of the last random variable given the past. For 
        stationary processes the limits exists in both cases and they are equal. The significance 
        of the entropy rate of a stochastic process arises from the Asymptotic Equipartition Property
        for a stationary ergodic process. The entropy rate is particularly well defined and easy
        to calculate for the stationary distribution of Markov chains, because if the Markov chain 
        is irreducible and aperiodic, then it has a unique stationary distribution on the states, 
        and any initial distribution tends to the stationary distribution as n -> ∞. In this case, 
        even though the initial distribution is not the stationary distribution, the entropy rate, 
        which is defined in terms of long term behaviour can be expressed with respect to the 
        stationary distribution.

        It is easy to see that a stationary random walk on a graph is time-reversible, that is, 
        the probability of any sequence of state is the same forward or backward. Rather surprisingly,
        the converse is also true, that is, any time-reversible Markov chain can be represented as a 
        random walk on an unidirected weighted graph.

        The entropy rate, hμ, of the Markov chain can be calculated. It is the average of
        transition uncertainty over all the states, it measures the information
        production rate in bits per time step. It is also the growth rate of the
        Shannon information in subsequences. In general, subsequences are not in a
        one-to-one correspondence with the Markov chain's state-to-state transition
        sequences. Nonetheless, it is a finite-to-one relationship. And so the
        Markov entropy rate is also the entropy rate of the original source. Thus,
        one a machine is reconstructed from a data stream, its entropy is an estimate
        of the underlying process's entropy rate. If the entropy rate is low then we
        gain certainty as the process proceeds. The entropy rate, is the rate in bits per 
        time step at which information is produced. The entropy rate is the expected number of 
        bits per symbol required to describe the process.

        An ϵ-machine's entropy rate hμ(M) is the transition uncertainty averaged over all 
        of the causal states. μ is a normalised invariant measure for a symbolic dynamical 
        system.

        The entropy rate is a key parameter associated with stochastic processes, information 
        sources, and dynamical systems. Roughly speaking, the entropy rate quantifies the average 
        uncertainty, disorder or irregularity generated by a process or system per 'time' unit and, 
        it is the primary subject of fundamental results in information and coding theory 
        (Shannon's noiseless coding theorem) and statistical mechanics (2nd law of thermodynamics).
        It is not surprising, therefore, that this notion, appropriately generalised and transformed, 
        is ubiquitous in many fields of mathematics and science when randomness or 'random-like' 
        behaviour is at the heart of the theory or model being studied.

        The metric of Shannon entropy rate of an information source is the rate of new information it 
        generates per unit time, just as the metric or Kolmogorov-Sinai entropy rate of a deterministic 
        dynamical system is a quantification of its pseudo-randomness or chaotic behaviour. 

        The entroy rate is an obvious statistic to estimate from an observed symbolic sequence. When 
        symbols are discretized, with an ever finer partition, from an orbit on the invariant 
        density of deterministic dynamics, the Shannon entropy rate of the symbolic sequence 
        divided by the discretization time step converges to the Kolmogorov-Sinai entropy, an invariant 
        of a dynamical system. A Kolmogorov-Sinai entropy greater than zero defines chaos. 

        Furthermore, for special sorts of partitions, termed "generating", the Shannon entropy equals 
        the KS-entropy even for low precision symbolic alphabets. Practical issues make the limit of 
        infinitely large alphabets inadvisable for KS-entropy estimation from finite sized data sets with 
        as the occupation in any bin tends to zero, making entropy estimation unreliable.

        If M is an axiom-A attractor, there is a prescription for constructing a partition 
        which is generating, and the equality of the metric entropy rate and the sum of 
        the positive Lyapunov characteristic exponents can be proven. In fact, whenever 
        an absolutely continuous invariant measure exists, a theorem due to Piesin shows 
        that the metric entropy of a diffeomorphism is equal to the sum of the positive 
        exponents. 

        The Markov matrix, a stochastic matrix, always has an eigenvalue that is
        equal to 1. The eigenvector associated with λ = 1 can be chosen to be strictly
        positive. All other eigenvalues have magnitude less than 1.

        A common type of Markov chain with transient states is an absorbing one. An
        absorbing Markov chain is a Markov chain in which it is impossible to leave
        some states, and any state could (after some number of steps, with
        positive probability) reach such a state. It follows that all non-absorbing
        states in an absorbing Markov chain are transient. A Markov chain that is
        aperiodic and positive recurrent is known as ergodic. Ergodic Markov chains
        are, in some senses, the processes with the "nicest" behaviour. A stochastic
        process contains states that may be either transient or recurrent; transience
        and recurrence describe the likelihood of a proces beginning in some state
        of returning to that particular state. There is some possibility (a nonzero
        probability) that a process beginning in a transient state will never return
        to that state. There is a guarantee that a process beginning in a recurrent
        state will return to that state. Transience and recurrence issues are central
        to the study of Markov chains and help describe the Markov chain's overall
        structure. The presence of may transient states may suggest that the Markov
        chain is absorbing, and a strong form of recurrence is necesary in an
        ergodic Markov chain. An absorbing state is a state for which the probability
        of returning to that state from itself is 1. Once a Markov chain is in an
        absorbing state, it will never leave the state. A state is known as recurrent
        or transient depending upon whether or not hte Markov chain will eventually
        return to it. A recurrent state is known as positive recurrent if it is
        expected to return within a finite number of steps, and null recurrent
        otherwise. A state is known as ergodic if it is positive recurrent and
        aperiodic. A Markov chain is ergodic if all its states are ergodic.

        The states of a Markov process are either recurrent, i.e., returned to 
        infinitely often, or transient, visited only finitely often with positive 
        probability. For us, here, the recurrent states represent the actual causal 
        structure of the process and, as such, they are what we are truly interested in. 
        the most important class of transient states, and indeed the only ones encountered 
        in practice, are the synchronization states, which can never be returned to, once 
        a recurrent state has been visited. The synchronization states represent observational 
        histories that are insufficient to fix the process in a definite recurrent state.  

        The states of a Markov process are either recurrent, i.e., returned to 
        infinitely often, or transient, visited only finitely often with positive 
        probability. For us, here, the recurrent states represent the actual causal 
        structure of the process and, as such, they are what we are truly interested in. 
        the most important class of transient states, and indeed the only ones encountered 
        in practice, are the synchronization states, which can never be returned to, once 
        a recurrent state has been visited. The synchronization states represent observational 
        histories that are insufficient to fix the process in a definite recurrent state.  

        Ergodic Markov chains have a unique stationary distribution, and absorbing
        Markov chains have stationary distributions with nonzero elements only in
        absorbing states. The stationary distribution gives information about the
        stability of a random process and, in certain cases, describesthe limiting
        behaviour of the Markov chain.

        The state space of a Markov chain can be partitioned into a set of non-
        overlapping communicating classes. States i and j are in the same
        communicating class if there is asome way of getting from state i to state j,
        AND there is some way of getting from state j to state i. It needn't be
        possible to get between i and j in a single step, but it must be possible
        over somenumber of steps to travel between them both ways. Mathematically,
        it is easy to show that the communicating relation is an equivalence relation
        which means that it partitions the sample space S into non-overlapping
        classes. States i and j are in the same communicating class if each state
        is accessible from the other.

        The stationary state probabilities are given by the left eigenvector of T,
        which corresponds to the eigenvalue that is equal to 1. Over the long run,
        no matter what the starting state was, the proportion of time the chain spends
        in state j is is approximately equal to l_eigvecs[j]. When there are multiple
        eigenvectors associated to an eigenvalue of 1, each such eigenvector gives
        rise to an associated stationary distribution. However, this can only occur
        when the Markov chain is reducible, i.e. has multiple communicating classes.

        The stationary distribution can then be used to calculate the information in
        the state-alphabet sequences. Not the alphabet associated with the measurements,
        but of the state-to-state transitions. This would measure the amount of memory
        in the source. Another term for this quantity would be the complexity, Cμ.
        Cμ is the Shannon entropy of stationary distribution which quantifies the
        uncertainty in bits for the causal states. The statistical complexity of a 
        state class in the average uncertainty (in bits) in the process's current state.
        This, in turn, is the same as the average amount of memory (in bits) that the
        process appears to retain about the past, given a chosen state class. That is,
        the graph complexity is a measure of an ϵ-machine's information processing 
        capacity in terms of the amount of information stored in the morphs. The complexity 
        is related to the mutual information of the past and future semi-infinite sequences 
        and to the convergence of the entropy rate. It can be interpreted as a measure of the 
        amount of mathematical work necessary to produce a fluctuation from asymptotic statistics. 
        In particular, it is the amount of information required to describe the behaviour at 
        a point, and equal the log of the effective number of causal states, i.e., of different 
        distribution of the future. It is the information contained in the causal states, given 
        past observations. It is the information about a system's causal state required for 
        maximal accurate prediction.

        The difficulty of performing some task is usually called its complexity. Thus the 
        Kolmogorov-Chaitin, (or algorithmic) complexity of a symbol sequence of length N 
        is essentially the length of the shortest computer program needed to generate the 
        sequence, divided by N. In cases where the sequence is an encoding of the trajectory 
        of a dynamical ystem, this is just the Kolmogorov-Sinai entropy: the difficulty 
        of specifying the entire sequence is just proportional to the amount of information 
        which has to be given in order to specify it, and this is proportional to h for long 
        time. But specifying a long symbol sequence is not the typical task associated with 
        a dynamical system. Apart from deducing the underlying equations of motion and its 
        parameters, the typical task is forecasting. We propose therefore the call 
        forecasting complexity of a dynamical system the difficulty involved in forecasting 
        it to the best possible extent. 

        Analogous to the case of complexity of computing functions, we could distinguis between 
        space and time complexity, depending on whether we consider limitations in storage or 
        in CPU time as more important. We shall do neither. Instead, we shall take as our primary 
        measure the average amount of (Shannon) information about the past sequence which has 
        to be kept at any particular time. Notice that this is inbetween space and time complexity:
        by concentrating on the average information instead of the maximal storage needed, we are 
        also sensitive to how often this information is fetched into fast memory. Thus the measure 
        is more appropriate than the common wortst-case measures, in situations where one has cheap 
        slow and expensive fast memory, and where one is sharing resources in a computer or in a 
        network of computers with other tasks. 

        The causal states are the minimal states that have a homogeneous distribution for
        the next sequence of symbols and are deterministic. 

        For completeness, note that there is an edge-complexity, Cμe that is the
        information contained in the asymptotic edge distribution. These quantities
        are not independent. From the principle of conservation of information leads
        to the relation Cμe = Cμ + hμ. 

        One of the primary roles of entropy in dynamical systems theory is that it is an invariant,
        which is to say that any two dynamical systems have the same metric entropy 
        if they are related by a isomorphism that preserves measure. 

        Topological entropy is related to measure entorpy by the variational principle 
        which asserts that for a continuous action on a compact space the topological 
        entropy equalts the supremum of the measure entropy taken over all the invariant 
        probability measures. Heuristically, the topological entropy of a dynamical 
        system measures the asymptotic growth rate of the number of resolvable orbits 
        (using a given measurement partition) whose initial conditions are all close. 
        Equivalently, the topological entropy quantifies the average time-rate of spreading 
        a subset over nearby subsets. This process is most easily illustrated by considering 
        a collection of subsets which form a "cover" of the state space M. The dynamic f 
        spreads a single cover element over other elements after some time t. The number of new 
        cover elements N(t) visited by points in the original cover element can be written as,
        N(t)~e^ht. Where h > 0 for chaotic dynamical systems. The supremum of h is obtained only 
        if the measurement partition is "good" in that there is an unambiguous correspondence 
        between orbits and symbol sequences. Only with such a good partition is the topological 
        entropy of the admissable symbol sequences obtained using partition. For the topological 
        entropy, the supremem is obtained only for special partitions; Kolmogorov proved that 
        the desired requirement is that the partition be generating. A partition is generating
        if, as the length of all sequences become large, the sequences label individual points.

        The topological entropy gives essentially no information about attractors, it indicates, 
        when positive, a sensitivity of the dynamical system to external noise.

        It is noted that, typically, the number of cells k required for a minimal generating 
        partition is not known a priori. A lower bound can be obtained if we know the topological 
        entropy of an attractor under consideration. In particular k >= 2^h, where the topological 
        entropy, h (in bits per iteration), quantifies the exponential rate of growth of the 
        number of observed "orbits" (symbol sequences) of increasing length. Alternatively, the 
        number of low period UPOs provides a lower bound on k which can sometimes be tighter than that 
        found using topological entropy. 

        In a sense the metric entropy is a generalization of the topological entropy: the 
        metric entropy also measures the asymptotic growth rate of the number of resolvable 
        orbits (using a given measurement partition) having close initial conditions, but 
        weights each orbit with its probability of occurence. That is, the Kolmogorov-Sinai 
        entropy is a quantitative measure of the impossibility of perfect forecasts. 

        Thus, there are only two independent quantities when modelling a source as a
        stochastic finite automaton. The entropy, hμ, as a measure of the diversity
        of patterns, and the complexity Cμ, as a measure of memory, have been taken
        as the two elementary "information processing" coordinates with which to
        analyze a range of sources. The net result of using just the complexity and 
        entropy rate is that the original equations of motion and any nonlinearity (bifurcation)
        parameter are simply forgotten. All that is of interest is how the complexity Cμ of 
        the data stream depends on the rate hμ of information production. The net result of using just the complexity and 
        entropy rate is that the original equations of motion and any nonlinearity (bifurcation)
        parameter are simply forgotten. All that is of interest is how the complexity Cμ of 
        the data stream depends on the rate hμ of information production.

        There is another set of quantities that derive from the skeletal structure
        of the machine. If we drop all probabilistic structure on the machine, we can
        calculate the growth rate of the raw number of sequences it produces - the
        topological entropy, which can be found from the connection matrix and its
        principal eigenvalue. Similarly, the state and transition complexities can
        easily be calculated. These can be used for processes with finite memory. The
        general notions without the finiteness restriction is Cμ and Cμe. The topological 
        complexity is the size of the minimal DFA description, or "program", required to 
        produce sequences in the observed measurement language of which s is a member. 
        This topological complexity counts all of the reconstructed states. Another 
        related topological complexity would count just the recurrent states. This is 
        the one commonly used in practice. The topological entropy;for a system given 
        by an iterated function, represents the exponential growth rate of the number 
        of distinguishabe orbits of the iterates.This is 
        the one commonly used in practice. The topological entropy;for a system given 
        by an iterated function, represents the exponential growth rate of the number 
        of distinguishabe orbits of the iterates.

        A measure of knowledge relaxation on finitary machines is given by the time-
        dependent finitary complexity. Where knowledge of a process consists of a 
        data stream, an agent's current model, how the information used to build the 
        model was obtained, and the method or algorithm which constructs the model.
        The trajectory of complexity captures the dynamics of knowledge relaxation.

        The finitary complexity is a measure of an ϵ-machine's information processing 
        capacity in terms of the amount of information stored in the morphs. It is 
        directly related to the mutual information of the past and future semi-infinite 
        sequences and the convergence of the entropy enstimates hα(L). It can be interpreted, 
        then, as a measure of the amount of mathematical work necessary to produce a fluctuation 
        form asymptotic statistics. The units for complexity measures are bits of information. 
        However, at this level we see that the complexity begins to more strongly reflect 
        the degree of computational capability and so we refer to the units as Turings, rather 
        than bits. At this low lever the difference between bits and Turings is not as dramatic 
        as at higher levels where each unit of machine structure is clearly associated with 
        sophisticated computation. 

        The finitary complexity is a measure of an ϵ-machine's information processing 
        capacity in terms of the amount of information stored in the morphs. It is 
        directly related to the mutual information of the past and future semi-infinite 
        sequences and the convergence of the entropy enstimates hα(L). It can be interpreted, 
        then, as a measure of the amount of mathematical work necessary to produce a fluctuation 
        form asymptotic statistics. The units for complexity measures are bits of information. 
        However, at this level we see that the complexity begins to more strongly reflect 
        the degree of computational capability and so we refer to the units as Turings, rather 
        than bits. At this low lever the difference between bits and Turings is not as dramatic 
        as at higher levels where each unit of machine structure is clearly associated with 
        sophisticated computation. 

        A measure of the "goodness of fit" that can be used to evaluate ϵ, μ, δ, and 
        the level L of subtree approximation can be given by the graph indeterminacy. 
        This quantity measure the degree of ambiguity in transitions between graph
        vertices. An ϵ-machine is reconstructable from L-level equivalence classes if 
        IG_L vanishes. Finite indeterminacy, at some given L, ϵ, μ, and δ indicates a 
        residual amount of extrinsic noise at the level of approximation.

        We may include an α parameterization of all the information and complexity quantities.
        This would include the α-order total Renyi entropy, or "free information", a 
        partition function, the Renyi-specific entropy (rate), i.e., entropy per measurement, etc.
        The parameter α has several interpretations, all of interest in the present context.
        From the physical point of view, α(=1-β) plays the role of the inverse temperature β
        int the statistical mechanics of spin systems. The spin states correspond to measurements 
        and a configuration of spins on a spatial lattice to a temporal sequence of 
        measurements. Just as the temperature increases the probability of different spin 
        configurations by increasing the number of available states, α accentuates different 
        subsets of measurement sequences in the asymptotic distribution. Particularly, as 
        α approaches 1, we accentuate the dynamics and not just the topology. For example, 
        any transient states will be removed from consideration. Just like temperature can 
        be viewed as a Lagrange multiplier from the perspective of thermodynamics, likewise α can be 
        seen as a Lagrange multiplier from the perspective of Bayesian inference. In the first 
        case the Lagrange multiplier, temperature, allows us to maximise the (Gibbs) entropy 
        subject to some constraints, like conservation of charge, probability, or energy. 
        That is, temperature maximises the probability distribution. Similarly, α tells us which 
        maximum entropy distribution corresponds with the maximum likelihood distribution of 
        observed (measured) cylinder probabilities. Temperature told us what the distribution 
        of energy levels would be, α tells us what the distribution over cyliders is. If temperature
        is small, then the system is dominated by the lowest energy levels. When α is 1, then the 
        system is dominated by the smallest set of probabilities over cylinders. The relation to 
        Bayesian inference is through the fact that when α is equal to 0 it is as if all knowledge 
        of the probabilistic structure of the machine has been removed, and we are in a maximal state 
        of ignorance. Since cylinder probabilities are closely associated with morph probabilities, and 
        thereby they specify our knowledge of the future, if they are removed from consideration, 
        then how we update our credances for future events (causal states) given a measurement value 
        is maximally unspecified. That is, we may know to which states we may transition to, but 
        not the likelihoods of transitioning. 

        Thus, the zeroth-order Renyi entropy gives the logarithm of the measure of the support set of 
        the probability density, and the Shannon entropy gives the logarithm of the size of the 
        "effective" support set. 

        Similarly, the partition function takes on an analogous role as in statistical mechanics, 
        where it is the counterpart of inverse temperature and also a Lagrange multiplier. Specifically,
        it is a normalisation constant, which doesn't distinguish between the different states (cylinders) 
        while ensuring the conservation of information according to the 2nd axiom  of probability and the 
        conservation of cylinders. 

        Following symbolic dynamics terminology, α = 0 will be referred to as the topological or 
        counting case; α = 1, as the metric or probabilistic case or high temperature limit. Varying 
        α moves continuously from topological to metric machines. Originally in his studies of 
        generalized information measures, Renyi introduces α as just this type of interpolation 
        parameter and noted that α-entropy has the character of a Laplace transform of a distribution. 
        here we have another requirement for α: it gives the proper algebra of trajectories in orbit 
        space. That is, α is necessary for computing measurement sequence probabilities from the 
        stochastic connection matrix Tα. Without it, products of Tα fail to distinguish distinct 
        sequences. 

        A central requirement in identifying models from observed data is that a particular
        inference methodology produces a sequence of hypotheses that converge to the
        converge to the correct one describing the underlying process. The complexity can 
        be used as a diagnotic for this since it is a direct measure of the size of the 
        hypothesized stochastic DFA at a given reconstruction cylinder length. The 
        identification method converges with increasing cylinder length if the rate of 
        change of complexity vanishes as the complexity vanishes as L is increased. If 
        the complexity vanishes, then the noisy dynamical system system has been identified. 
        If it does not vanish in the limit L-> ∞, then the complexity is a measure of the 
        rate of divergence of the model size and so quantifies a higher level of computational 
        complexity. In this case, the model basis must be augmented in an attempt to find 
        a finite description at some higher level. In other words, while the DFA may be 
        a good fit for the data, it may not necessarily be the best model for the system.

        The machines alse capture the information flow within the given data stream. If state B 
        follows state A then as far as the observer is concerned A is a cause of B and B is one 
        effect of A. In this case, we say that information flows from A to B. The amount of 
        information that flows is the negative logarithm of the connecting transition probability 
        -log(2, Deus.T[A, B])

        HShannon measues the number of distinct sequences (possible futures). That number increases 
        if there is branchin as one moves down the tree and forward in time. At the other end, the 
        Hartley entropy is given simply by the total number of distinct sequences independent of their 
        probability. If the probability distributin is uniform on the nonzero probability cylinders then 
        these two entropies are equal. Any difference is thus a measure of deviation of the cylinder from 
        uniformity. The latter observation leads to a parametrized generalization of the entropy introduced 
        by Renyi. This we put into a statistical mechanical formalism by defining a partition function for the 
        tree. 

        The average branching rate in the tree measures the growth rate of the number of new sequences 
        of increasing length. And as such it is a measure of unpredictability in that a periodic process will 
        at some length give rise to no more new cylinders and a random one will. The Renyi specific entropy, i.e. 
        entropy per measurement, can be approximation from the L-cylinder distribution by hα. The growth rate of 
        total Shannon entropy is often referred to in information theory as the source entropy and in dynamical 
        systems as the metric entropy. The corresponding Hartley entropy growth rate is called the topological entropy. 

        If a tree representation is good, then the information gain, or entropy rate, at some depth will vanish. 
        This indicates that no further information need be stored to represent the process. This happens for a 
        periodic process for trees deeper than the period. If the process is chaotic, with positive entropy, then 
        the information contained in the tree representation will grow exponentially fast with modelling longer 
        subsequences in the data stream. This indicates that the tree representation is inadequate and suggests  
        that the modeler innovate a new class of representations. 

        HShannon measues the number of distinct sequences (possible futures). That number increases 
        if there is branchin as one moves down the tree and forward in time. At the other end, the 
        Hartley entropy is given simply by the total number of distinct sequences independent of their 
        probability. If the probability distributin is uniform on the nonzero probability cylinders then 
        these two entropies are equal. Any difference is thus a measure of deviation of the cylinder from 
        uniformity. The latter observation leads to a parametrized generalization of the entropy introduced 
        by Renyi. This we put into a statistical mechanical formalism by defining a partition function for the 
        tree. 

        The average branching rate in the tree measures the growth rate of the number of new sequences 
        of increasing length. And as such it is a measure of unpredictability in that a periodic process will 
        at some length give rise to no more new cylinders and a random one will. The Renyi specific entropy, i.e. 
        entropy per measurement, can be approximation from the L-cylinder distribution by hα. The growth rate of 
        total Shannon entropy is often referred to in information theory as the source entropy and in dynamical 
        systems as the metric entropy. The corresponding Hartley entropy growth rate is called the topological entropy. 

        If a tree representation is good, then the information gain, or entropy rate, at some depth will vanish. 
        This indicates that no further information need be stored to represent the process. This happens for a 
        periodic process for trees deeper than the period. If the process is chaotic, with positive entropy, then 
        the information contained in the tree representation will grow exponentially fast with modelling longer 
        subsequences in the data stream. This indicates that the tree representation is inadequate and suggests  
        that the modeler innovate a new class of representations. 

        These quantities are probabilistic, and referred to Turing machines with a 
        random internal register. 
    =#

    Deus.IG = 0

    num_edges = 0
    num_states = length(Deus.Ts)
    T = Array{Float64, 2}(undef, num_states, num_states)
    T = fill!(T, 0.0)
    Deus.T = cat(T, dims = 2)

    for v in 1:num_states

        if Deus.Ts[v][2, 1] == -1
            Surgeon(Deus, v)
        end

        #normalise probabilities - only necessary if IG does not vanish
        pt = sum(Deus.Ts[v][3, :])  
        if pt != 1
            Deus.Ts[v][3, :] =  Deus.Ts[v][3, :]./pt
        end  

        num_trans = length(Deus.Ts[v][1,:])

        for vd in 1:num_trans

            state_to = Int(Deus.Ts[v][1, vd])
            Deus.T[v, state_to] = Deus.T[v, state_to] + Deus.Ts[v][3, vd]
        end
    end

    Deus.I = -log.(2, Deus.T) # Information flow between nodes
    Deus.eigval = eigvals(Deus.T) # The eigenvalues for left and right eigenvectors are equal
    Deus.l_eigvecs = eigvecs(transpose(Deus.T)) # the left eigenvectors are the right eigenvectors of the transpose
    Deus.r_eigvecs = eigvecs(Deus.T)
    Deus.D = Diagonal(Deus.eigval)

    T0 = Float64.(Deus.T .!= 0) # the connectivity / adjacency matrix of T
    eigval_0 = eigvals(T0)
    l_eigvecs_0 = eigvecs(transpose(T0))
    _, id0 = findmax(real.(eigval_0))
    pv_0 = abs.(l_eigvecs_0[1:end, id0])/abs(sum(l_eigvecs_0[1:end, id0]))

    Deus.C = log(2, sum(pv_0 .!= 0)) # state topological complexity - only counting recurrent states

    _, index = findmax(real.(Deus.eigval))

    if real(Deus.eigval[index]) >= 0.99

        Deus.pv = abs.(Deus.l_eigvecs[1:end, index])/abs(sum(Deus.l_eigvecs[1:end, index]))

        for v in 1:num_states

            if pv_0[v] != 0.0

                trans = unique(Deus.Ts[v][1:2, :], dims = 2)
                num_trans = length(Deus.Ts[v][1,:])

                num_edges = num_edges + size(trans, 2)

                Deus.h = Deus.h + pv_0[v]*size(trans, 2)

            elseif Deus.pv[v] != 0

                trans = unique(Deus.Ts[v][1:2, :], dims = 2)
                num_trans = length(Deus.Ts[v][1,:])
            end

            if Deus.pv[v] != 0

                Deus.Cμ = Deus.Cμ - Deus.pv[v]*log(2, Deus.pv[v])
                
                syms = unique!(Deus.Ts[v][2, :])
                psv = Array{Float64, 1}(undef, length(syms))

                for s in eachindex(syms)
                    sloc = findall( ==(syms[s]), Deus.Ts[v][2, :])
                    psv[s] = sum(Deus.Ts[v][3, sloc])
                end
                
                for vd in axes(trans, 2)

                    vloc = findall(i -> all(j -> trans[:, vd][j] == Deus.Ts[v][1:2, :][j,i], 1:2), 1:num_trans)

                    pt = sum(Deus.Ts[v][3, vloc])
                    pe = Deus.pv[v]*pt
 
                    Deus.hμ = Deus.hμ - pe*log(2, pt)
                    Deus.Cμe = Deus.Cμe - pe*log(2, pe)

                    sym = Deus.Ts[v][2, vd]
                    sloc = findfirst( ==(sym), syms)

                    Deus.IG = Deus.IG - pe*log(2, pt/psv[sloc])
                end
            end
        end

        if Deus.h != 0
            Deus.h = log(2, Deus.h)
        end

        Deus.Ce = log(2, num_edges) # transition topological complexity
    else

        println("\nError. The eigenvalue with value of 1 could not be found
        in the Markov Matrix.")
    end

    return nothing
end

function Load_values(Deus::ϵ_Machine)

    Deus.T = Deus.Tα[end]
    Deus.I = Deus.Iα[end]
    Deus.D = Deus.Dα[end]
    Deus.pv = Deus.pv_α[end]

    Deus.h = Deus.hα[1]
    Deus.hμ = Deus.hα[end]
    Deus.C = Deus.Cα[1]
    Deus.Cμ = Deus.Cα[end]
    Deus.Ce = Deus.Cαe[1]
    Deus.Cμe = Deus.Cαe[end]

    return nothing
end

function Markov_Evolution(Deus::ϵ_Machine, n; start_ic = zeros(1, length(Deus.Ts)))

    # n is the number of steps

    num_states = length(Deus.Ts)
    index = findfirst(==(1), Deus.C_states[1,:]) # finding starting state

    if index !== nothing && length(start_ic) == num_states

        if sum(start_ic) == 0

            start_ic[index] = 1.0
        end

        if  det(Deus.r_eigvecs) != 0
            inv_r_eigenvecs = inv((Deus.r_eigvecs))
            #inv_l_eigenvecs = inv(Deus.l_eigvecs)
            Dn = Diagonal((Deus.eigval).^n)

            state = start_ic*(Deus.r_eigvecs)*(Dn)*inv_r_eigenvecs
            #state = Deus.l_eigvecs*(Dn)*inv_l_eigenvecs*transpose(start_ic) # equivalent
        else
            state = start_ic
            for i in 1:n-1
                
                state = state*Deus.T                
            end
        end
    else
        state = 0
        println("\nMarkov Evolution. Initial state not valid.")
    end

    return real.(state)
end

function Complexity_Series(Deus::ϵ_Machine)

    # n is the number of steps

    N = length(Deus.t_m)
    num_states = length(Deus.Ts)
    start_ic = zeros(1, length(Deus.Ts))
    index = findfirst(==(1), Deus.C_states[1,:]) # finding starting state

    if index !== nothing && length(start_ic) == num_states

        if sum(start_ic) == 0

            start_ic[index] = 1.0
        end

        state_dist = start_ic
        Deus.Cμ_t[1] = 0
        for i in 2:N
            
            state_dist = state_dist*Deus.T
            Deus.Cμ_t[i] = 0

            for v in 1:num_states
    
                if state_dist[v] != 0
    
                    Deus.Cμ_t[i] = Deus.Cμ_t[i] + state_dist[v]*log(2, state_dist[v])
                end
            end  
            
            Deus.Cμ_t[i] = -Deus.Cμ_t[i]
        end
    else
        println("\nMarkov Evolution. Initial state not valid.")
    end

    return nothing
end

function χ2_test(dist_1, dist_2, N1, N2; α = 0.05, IND = 2)

    # Perform a Chi^2 test, if the probabilities and number of occurrences are given
    # IND ∈ [0,1,2] sets accuracy: IND=0 means 14 significant digits accuracy, 
    # IND = 1 means 6 significant digit, and IND = 2 means only 3 digit accuracy

    K = length(dist_1) - 1 # degrees of freedom
    
    if K > 1

        pa = 0.0
        for i in eachindex(dist_1)
            pa = pa + (dist_1[i]*N1)^2/(dist_1[i]*N1 + dist_2[i]*N2)
        end

            χ_2 = (pa - N1^2/(N1 + N2))/(N1*N2/(N1 + N2)^2)

        if χ_2 < 0 
            χ_2 = 0
        end

        p, _ = (1/gamma(K/2)).*gamma_inc(K/2 ,(χ_2)/2, IND)
    else

        p = maximum(abs.(dist_1 .- dist_2))
    end

    return p < α #outcome # true or false
end

function Surgeon(Deus::ϵ_Machine, state_from)

    Deus.Ts[state_from] = Matrix{Float64}(undef, 3, 0)
    node = Deus.C_states[1, state_from]

    num_children = length(Deus.Tree.Nodes[node].child_vals)

    perm = sortperm(Deus.Tree.Nodes[node].child_vals)

    symbols = Deus.Tree.Nodes[node].child_vals[perm]
    state_Pr_c = Deus.Tree.Nodes[node].Pr_c[perm]
    ancest = Deus.Tree.Nodes[node].parent
    cn = Deus.Tree.Nodes[node].cn

    state_ref = The_Wire(Deus, symbols, ancest, state_Pr_c, cn)

    if state_ref != -1

        perm = sortperm(Deus.Ts[state_ref][1, :])
        state_to = Deus.Ts[state_ref][1, perm]

        #parent_state = Deus.Tree.Nodes[ancest].state
        #fnd = findall(x->x == (state_from), Deus.Ts[parent_state][1,:])

        #Deus.Ts[parent_state][1, fnd] = ones(length(fnd))*state_ref
        
        #deleteat!(Deus.Ts, state_from)
    end

    for n in 1:num_children

        if state_ref ==  -1

            Deus.Ts[state_from] = cat(Deus.Ts[state_from], [Deus.start_state; symbols[n]; state_Pr_c[n]], dims = 2)
        else

            Deus.Ts[state_from] = cat(Deus.Ts[state_from], [state_to[n]; symbols[n]; state_Pr_c[n]], dims = 2)
        end
    end

    #= 
        # Identifying the transitions of any dangling states with the start state, since both have elements of ignorance
        # Or they should be removed

        Deus.Ts[state_from] = Matrix{Float64}(undef, 3, 0)
        Node = Deus.C_states[1, state_from]

        num_children = length(Deus.Tree.Nodes[Node].child_vals)
        child_locs = Deus.Tree.Nodes[Node].child_locs

        for n in 1:num_children

            symbol = Deus.Tree.Nodes[child_locs[n]].value

            state_to = Deus.start_state
            state_Pr_c = Deus.Tree.Nodes[Node].Pr_c[n]

            Deus.Ts[state_from] = cat(Deus.Ts[state_from], [state_to; symbol; state_Pr_c], dims = 2)
        end
    =#

    return nothing
end

function The_Wire(Deus::ϵ_Machine, symbols, ancest, Pr_c, cn)
    
    if ancest != 0

        perm = sortperm(Deus.Tree.Nodes[ancest].child_vals)

        sorted_vals = Deus.Tree.Nodes[ancest].child_vals[perm]

        val_test = (sorted_vals == symbols)

        if length(Pr_c) > 1 && val_test

            ancest_dist = Deus.Tree.Nodes[ancest].Pr_c[perm]
            ancest_cn = Deus.Tree.Nodes[ancest].cn
            Pr_test = χ2_test(Pr_c, ancest_dist, cn, ancest_cn; α = Deus.δ) 

        elseif val_test

            Pr_test = abs(Deus.Tree.Nodes[ancest].Pr_c[1] - Pr_c[1]) <= Deus.δ
        end

        if !val_test

            # go up one more node until we hit the root
            ancest = Deus.Tree.Nodes[ancest].parent
            state_ref = The_Wire(Deus, symbols, ancest, Pr_c, cn)

        elseif !Pr_test

            ancest = Deus.Tree.Nodes[ancest].parent
            state_ref = The_Wire(Deus, symbols, ancest, Pr_c, cn)
        else

            state_ref = Deus.Tree.Nodes[ancest].state
        end

    else

        state_ref = -1
    end

    return state_ref
end

function Permutation_Entropy(Deus::ϵ_Machine, D)

    #= Theory:
        The permutation entropy appears to be very similar to the Lyapunov exponents.
        Most importantly, it yields meaningful results in the presence of observational 
        and dynamical noise. Permutation entropy is an appropriate complexity measure for a  
        chaotic time series. In contrast with all known complexity measures, a small noise 
        does not essentially change the complexity for arbitrary real-world time series. 
        Since the method is extremely fast and robust, it seems preferable when there are 
        huge data sets and no time for preprocessing and fine-tuning of parameters. 
        
        Another advantage is that the symbol sequence comes naturally from the data, x, 
        without further model assumptions. Partitions are found by comparison of 
        neighbouring values of xt (i.e. ordinality). The entropies are calculated for d
        ifferent embedding dimensions n (alphabet size). For practical purposes, n is 
        recommended to be chosen between 3 and 7. 

        The lower bound on the permutation entropy is 0 and the upper bound is log(n!). 
        Hence, we can normalise the upper bound to 1. The lower bound is attained for an 
        increasing or decreasing sequence of values, and the upper bound for a completely 
        random system (i.i.d. sequence) wehere all n! possible permutations appear with 
        the same probability. The time series presents some sort of dynamics when the 
        normalised entropy is less that 1. 

        It has been shown that for piecewise monotone interval maps, hpe, converges to the 
        Kolmogorov-Sinai entropy rate with n -> ∞, but this convergence is rather slow.

        Permutation entropy quantifies the diversity of possible orderings of the values a 
        random or deterministic system can take, as Shannon entropy quantifies the diversity 
        of values. The metric and permutation entropy rates - measures of new disorder per 
        new observed value - are equal for ergodic finite-alphabet information sources 
        (discrete time stationary stochastic processes). The same holds for deterministic 
        dynamical systems defined by ergodic maps on n-dimensional intervals. 
    =#

    println("\n____________________________________________________")
    println("Permutation Entropy")

    max_H = log(2, factorial(D)) # upper bound
    Deus.k = size(Deus.x, 2)
    divs = Deus.k - D + 1 # divisor

    dims = size(Deus.x, 1) # dimensions of state space

    rolling = Array{Float64, 1}(undef, D)
    s_pe = Array{Float64, 1}(undef, D)

    for d in 1:dims

        #N = divs*D
        Deus.Tree = Parse_Tree(Deus.k + 1, D, 0, 0.0) # initialising an empty tree

        for i in 1:Deus.k 

            rolling[1:end-1] = rolling[2:end]

            if i < Deus.k
                rolling[end] = Deus.x[d, i + 1] .- Deus.x[d, i]
            end

            if i >= D 

                s_pe[1] = 0.0
                s_pe[2] = rolling[1]

                for j in 2:(D-1)

                    s_pe[j+1] = s_pe[j] + rolling[j]
                end

                #s = sortperm.(eachrow(s_pe))
                s = sortperm(s_pe) .- 1

                add_Nodes(Deus, s, i - D + 1)
            end
        end

        Deus.Hpe[d] = 0.0

        for j in eachindex(Deus.Tree.Cyls)

            p_π = length(Deus.Tree.Cyls[j])/divs
            Deus.Hpe[d] = Deus.Hpe[d] - p_π*log(2, p_π)
        end
        Deus.Hpe[d] = Deus.Hpe[d]/max_H
        Deus.hpe[d] = Deus.Hpe[d]/(D - 1)

        println("\nDimension = ", d)
        println("Permutation Entropy = ", round(Deus.Hpe[d], digits = 3), " bits")
        println("Permutation Entropy per symbol = ", round(Deus.hpe[d], digits = 3), " bits")
    end

    return nothing
end

function Cranking(Deus::ϵ_Machine, x_in, μ_s)

    Deus.x = x_in

    Sampling(Deus, μ_s)

    Permutation_Entropy(Deus, 6)

    #Symbolic_Shadowing(Deus)

    #= Tree_Isomorphism(Deus)

    #Statistical_Mechanics(Deus)
    Parametric_Statistical_Mechanics(Deus) # same as above but paramatrized to order α
    Load_values(Deus)

    Complexity_Series(Deus) # time evolution of complexity - knowledge relaxation =#

    return nothing
end

#_______________________________________________________________________________
# Graphs functions

function Circular_Layout(p, ax, L)
    # gives the coordinates to the nodes such that they are laid out in a circle
    radius = 5
    # circular layout - square lattice
    for i in 1:L
        # graph coordinates
        p[:node_pos][][i] = (radius*cos((i-1)*2*π/(L)), radius*sin((i-1)*2*π/(L)))
    end
    p[:node_pos][] = p[:node_pos][]
    limits!(ax, -radius*1.5, radius*1.5, -radius*1.5, radius*1.5)

    return nothing
end

function Plane_Layout(p, ax, L)
    # gives the coordinates to the nodes such that they are laid out in a plane
    l = 1
    d = convert(Int64, round(sqrt(L)))
    f = d - 1 
    for i in 1:L
        # graph coordinates
        x = l*(i-1)%d - f*l/2
        y = -l*floor((i-1)/d) + f*l/2
        p[:node_pos][][i] = (x, y)
    end
    p[:node_pos][] = p[:node_pos][]
    limits!(ax, -f*l/2 - l/2, f*l/2 + l/2, -f*l/2 - l/2, f*l/2 + l/2)

    return nothing
end

function AddNode(CM, loc, nodes)

    # Adds new unconnected nodes to the system (if it is not already there)

    if size(CM, 1) < node

        increase = node - size(CM, 1)
        L_or = size(CM, 1)
        L_in = L_or + increase
        new_CM = Array{Int64, 2}(undef, L_in, L_in)
        new_CM = fill!(new_CM, 0)
        println("CM = ")
        display(CM)

        #= for i in 1:L_or
            for j in 1:L_or
                if i < loc && j < loc

                    new_CM[i,j] = CM[i,j]
                elseif i >= loc && j < loc

                    new_CM[i+increase,j] = CM[i,j]
                elseif i < loc && j >= loc

                    new_CM[i,j+increase] = CM[i,j]
                elseif i >= loc && j >= loc

                    new_CM[i+increase, j+increase] = CM[i,j]
                end
            end
        end =#

        return new_CM
    else
        return CM
    end

    return CM
end

function NewCM(CM, loc, nodes)

    new_CM = Array{Int64, 2}(undef, size(CM, 1)+nodes, size(CM, 1)+nodes)
    new_CM = fill!(new_CM, 0)

    new_CM[1:loc, 1:loc] = CM[1:loc, 1:loc]
    new_CM[1:loc, (loc + nodes + 1):end] = CM[1:loc, loc+1:end]
    new_CM[(loc + nodes + 1):end, 1:loc] = CM[loc+1:end, 1:loc]
    new_CM[(loc + nodes + 1):end, (loc + nodes + 1):end] = CM[loc+1:end, loc+1:end]

    return new_CM
end

function AddEdge(M, node1, node2)
    # Adds a new edge to the system, and creates the nodes if they don't exist
    if size(M,1) < node1 && size(M,1) < node2
        M = AddNode(M, maximum(node1,node2))
    end

    M[node1,node2] = 1
    M[node2,node1] = 1 # bidirectional graph

    return M
end

function GetNeighbours(M, node)

    deg = sum(M[node,:])

    N = Array{Int64, 1}(undef, Int(deg))

    count = 1
    for i in 1:size(M,1)

        if M[node, i] == 1
            N[count] = i
            count = count + 1
        end
    end

    return N
end

function GetMoore(M, node, torus)

    #=
        Returns a vector with locations of associated nodes in a Moore neighbourhood 
        with a range of 1.

        A Moore neighbourhood is a square-shaped neighbourhood that can be used to 
        define a set of cells surrounding a given cell that may affect the evolution
        of a two-dimensional cellular automaton on a square grid.
    =#

    L = size(M, 1)
    d = convert(Int64, round(sqrt(L)))
    N = Array{Int64, 1}(undef, 0)

    # next node
    if node%d != 0

        nxtn = node + 1
        N = [N; nxtn]

        # down
        if nxtn + d <= L
            N = [N; nxtn + d]
        elseif torus
            k = nxtn%d
            if k == 0
                k = d
            end
            N = [N; k]
        end

        # up
        if nxtn - d >= 1
            N = [N; nxtn - d]
        elseif torus
            N = [N; d*(d - 1) + nxtn]
        end

    elseif torus

        nxtn = node - d + 1
        N = [N; nxtn]

        # next node down
        if nxtn + d <= L
            N = [N; nxtn + d]
        else
            k = nxtn%d
            if k == 0
                k = d
            end
            N = [N; k]
        end

        # next node up
        if nxtn - d >= 1
            N = [N; nxtn - d]
        else
            N = [N; d*(d - 1) + nxtn]
        end
    end

    # previous node
    if node%d != 1

        prvn = node - 1
        N = [N; prvn]

        # down
        if prvn + d <= L
            N = [N; prvn + d]
        elseif torus
            k = prvn%d
            if k == 0
                k = d
            end
            N = [N; k]
        end

        # up
        if prvn - d >= 1
            N = [N; prvn - d]
        elseif torus
            N = [N; d*(d - 1) + prvn]
        end

    elseif torus

        prvn = node + d - 1
        N = [N; prvn]

        # next node down
        if prvn + d <= L
            N = [N; prvn + d]
        else
            k = prvn%d
            if k == 0
                k = d
            end
            N = [N; k]
        end

        # next node up
        if prvn - d >= 1
            N = [N; prvn - d]
        else
            N = [N; d*(d - 1) + prvn]
        end

    end

    # node down
    if  node + d <= L
        N = [N; node + d]
    elseif torus
        k = node%d
        if k == 0
            k = d
        end
        N = [N; k]
    end

    # node up
    if  node - d >= 1
        N = [N; node - d]
    elseif torus
        N = [N; d*(d - 1) + node]
    end

    return N
end

function SquareLattice(L; torus = false)

    # creates a graph connected in a square lattice
    # if torus is true then we have periodic boundary conditions

    d = convert(Int64, floor(sqrt(L)))
    L = d^2 # number of nodes for square
    
    M = Array{Float64, 2}(undef, L, L)
    M = fill!(M, 0)

    for i in 1:L

        if i%d != 0

            M[i, i + 1] = 1
        elseif torus && M[i, i - d + 1] == 0 && M[i - d + 1, i] == 0
            
            M[i, i - d + 1] = 1
        end

        if  i + d <= L

            M[i, i + d] = 1

        elseif torus

            k = i%d
            if k == 0
                k = d
            end
            if M[i, k] == 0 && M[k, i] == 0
                M[i, k] = 1
            end
        end

    end
    M = M + transpose(M)

    return M, L
end

function CircularGraph(M, L)
    #=
    In graph theory, a cycle graph Cn, sometimes simply known as an n-cycle, is
    a graph on n nodes containing a single cycle through all nodes. A different sort
    of cycle graph, termed a group cycle graph is a graph which shows cycles of a
    group as well as the connectivity between the group cycles
    =#

    M = Array{Float64, 2}(undef, L, L)
    M = fill!(M, 0)

    for i in 1:L

        if i < L
            M[i,i+1] = 1
        else
            M[1,i] = 1
        end
    end
    M = M + transpose(M)

    return M
end

function SmallWorld(num_nodes; Z = 2, p = 0.2, num_sources = num_nodes)

    # Z, the short edges connecting it to its nearest neighbours, up to a distance
    # of Z/2. Coordination number, if Z = num_nodes, then we have a fully connected graph.

    #p the fraction of random connections

    # The L nodes in a small world networks are arranged around a circle

    # There are p x L x Z/2 shortcuts added to the network, which connect nodes
    # at random

    #=
        Newman and Watts contend that the model displays a critical point with a divergent
        characteristic length as p tends to zero. They argue that the small-world model
        has a critical point in the limit where the density of shortcuts tends to zero.
        This phase transition gives rise to specific finite-size scaling behaviour in the
        region close to the transition.

        In the limit where the number of nodes N becomes large and the number of shortcuts,
        pLZ/2 stays fixed, this network has a nice limit where the distance is measured in
        radians Δθ around the circle. Dividing the average length by l(p = 0)~L/2Z, essentially
        does this, since Δθ = πZl/L. In this limit, the average bond length should be a
        function only of M = pLZ/2
    =#

    num_loads = num_nodes - num_sources
    
    if num_sources > num_loads
        num_sources = num_nodes
    end

    distance = convert(Int64, round(Z/2))
    CM = Array{Int64, 2}(undef, num_sources, num_sources)
    CM = fill!(CM, 0)

    cablecount = 1
    load_cnt = 0

    if num_sources == num_nodes

        for i in 1:num_sources

            # add the nearest neighbour connections
            # each node i should be connected to nodes mod((i - Z/2),L),... mod((i + Z/2),L)
            for j in 1:distance

                if i + j <= num_sources

                    CM[i, i + j] = cablecount
                    cablecount += 1

                elseif CM[i + j - num_sources, i] == 0 

                    CM[i + j - num_sources, i] = cablecount
                    cablecount += 1
                end
            end
        end

    elseif num_sources <= num_loads

        for i in 1:num_sources

            # add the nearest neighbour connections
            # each node i should be connected to nodes mod((i - Z/2),L),... mod((i + Z/2),L)

            for j in 2:distance

                if i + j <= num_sources

                    CM[i, i + j] = cablecount
                    cablecount += 1

                elseif CM[i + j - num_sources, i] == 0 

                    CM[i + j - num_sources, i] = cablecount
                    cablecount += 1
                end
            end
        end
    end

    # add the random connections
    for i in 1:convert(Int64, round(p*num_sources*Z/2))

        node1 = convert(Int64, round((num_sources-1)*rand() + 1))
        node2 = convert(Int64, round((num_sources-1)*rand() + 1))

        while CM[node1, node2] >= 1 || CM[node2, node1] >= 1 || node1 == node2

            node1 = convert(Int64, round((num_sources-1)*rand() + 1))
            node2 = convert(Int64, round((num_sources-1)*rand() + 1))

            if sum(CM) >= num_sources*(num_sources - 1)/2
                break
            end
        end

        if node2 > node1 && CM[node1,node2] < 1

            CM[node1,node2] = cablecount
            cablecount += 1

        elseif node1 > node2 && CM[node2,node1] < 1
            
            CM[node2,node1] = cablecount
            cablecount += 1
        end
    end

    if num_sources <= num_loads

        ratio = Int(floor(num_loads/num_sources))
        remainder = num_loads%num_sources

        for i in 1:num_sources

            if i <= remainder
                ad_ratio = ratio + 1
            else
                ad_ratio = ratio
            end

            CM = NewCM(CM, size(CM, 1), ad_ratio)

            # Add load nodes - starting
            node1 = i
            node2 = num_sources + load_cnt + 1

            if node2 > num_nodes
                node2 = 1
            end

            if node1 < node2
                CM[node1, node2] = cablecount
            else
                CM[node2, node1] = cablecount
            end

            cablecount += 1

            # Add edges between loads
            for j in 1:ad_ratio-1

                node1 = num_sources + load_cnt + j
                node2 = num_sources + load_cnt + j + 1

                if node1 < node2
                    CM[node1, node2] = cablecount
                else
                    CM[node2, node1] = cablecount
                end

                cablecount += 1
            end

            load_cnt += ad_ratio

            # Add load nodes - ending
            node1 = num_sources + load_cnt
            node2 = i + 1

            if node2 > num_sources
                node2 = 1
            end

            if node1 < node2
                CM[node1, node2] = cablecount
            else
                CM[node2, node1] = cablecount
            end

            cablecount += 1
        end
    end

    CM = CM - transpose(CM)

    return CM, cablecount - 1
end

function MG_SmallWorld(num_sources; Z = 2, p = 0.2)

    # Z, the short edges connecting it to its nearest neighbours, up to a distance
    # of Z/2. Coordination number, if Z = num_nodes, then we have a fully connected graph.

    #p the fraction of random connections

    # The L nodes in a small world networks are arranged around a circle

    # There are p x L x Z/2 shortcuts added to the network, which connect nodes
    # at random

    #=
        Newman and Watts contend that the model displays a critical point with a divergent
        characteristic length as p tends to zero. They argue that the small-world model
        has a critical point in the limit where the density of shortcuts tends to zero.
        This phase transition gives rise to specific finite-size scaling behaviour in the
        region close to the transition.

        In the limit where the number of nodes N becomes large and the number of shortcuts,
        pLZ/2 stays fixed, this network has a nice limit where the distance is measured in
        radians Δθ around the circle. Dividing the average length by l(p = 0)~L/2Z, essentially
        does this, since Δθ = πZl/L. In this limit, the average bond length should be a
        function only of M = pLZ/2
    =#

    distance = convert(Int64, round(Z/2))
    CM = Array{Int64, 2}(undef, num_sources, num_sources)
    CM = fill!(CM, 0)

    cablecount = 1

    if num_sources > 1

        for i in 1:num_sources

            # add the nearest neighbour connections
            # each node i should be connected to nodes mod((i - Z/2),L),... mod((i + Z/2),L)
            for j in 1:distance

                if i + j <= num_sources

                    CM[i, i + j] = cablecount
                    cablecount += 1

                elseif CM[i + j - num_sources, i] == 0 

                    CM[i + j - num_sources, i] = cablecount
                    cablecount += 1
                end
            end
        end

        # add the random connections
        for i in 1:convert(Int64, round(p*num_sources*Z/2))

            node1 = convert(Int64, round((num_sources-1)*rand() + 1))
            node2 = convert(Int64, round((num_sources-1)*rand() + 1))

            while CM[node1, node2] >= 1 || CM[node2, node1] >= 1 || node1 == node2

                node1 = convert(Int64, round((num_sources-1)*rand() + 1))
                node2 = convert(Int64, round((num_sources-1)*rand() + 1))

                if sum(CM) >= num_sources*(num_sources - 1)/2
                    break
                end
            end

            if node2 > node1 && CM[node1,node2] < 1

                CM[node1, node2] = cablecount
                cablecount += 1

            elseif node1 > node2 && CM[node2, node1] < 1
                
                CM[node2, node1] = cablecount
                cablecount += 1
            end
        end

    end

    #= CM = NewCM(CM, size(CM, 1), 2*num_sources) 

    for i in 1:num_sources

        # Add controllable Load

        node1 = i
        node2 = i + num_sources
        CM[node1, node2] = cablecount
        cablecount += 1

        # Add Static Load

        node1 = i
        node2 = i + 2*num_sources
        CM[node1, node2] = cablecount
        cablecount += 1
    end
 =#

    CM = NewCM(CM, size(CM, 1), num_sources) 

    for i in 1:num_sources

        # Add Static Load

        node1 = i
        node2 = i + num_sources
        CM[node1, node2] = cablecount
        cablecount += 1
    end

    CM = CM - transpose(CM)

    return CM, cablecount - 1
end

function Barabasi_Albert(L; m = 1, α = 0)

    #=
        The Barabasi-Albert model is an algorithm for generating random scale-free networks
        using a preferential attachment mechanism. Several natural and human-made systems,
        including the Internet, the World-wide-web, and some social networks are thought
        to be approximately scale-free and certainly contain few nodes (called hubs) with
        unusually high degree as compared to other nodes of the network.

        The network can begin with an initial connected network of m0 nodes.

        New nodes are added to the network one at a time. Each node is connected to
        m<= m0 existing nodes with a probability that is proportional to the number of
        links that the existing nodes already have.

        In this case m = 1 or m = 2. m0 is equal to 2

        if α is not equal to 1 then we have the more general non-linear preferential
        attachment (NLPA) model. If 0 < α < 1, NLPA is referred to as "sub-linear" and
        the degree distribution of the network tends to a stretched exponential
        distribution. if α > 1, NLPA is referred to as "super-linear" and a small
        number of nodes connect to almost all other nodes in the network. For both
        α < 1 and α > 1, the scale free property of the network is broken in the
        limit of infinite system size.
    =#

    CM = Array{Int64, 2}(undef, L, L)
    CM = fill!(CM, 0)

    CM[1,2] = 1 #just connecting the first node with the second
    CM[2,1] = -1

    cablecount = 2

    for i in 3:L

        m_count = 0

        while m_count < m

            for j in 1:i-1

                k = sum(abs.(CM[j, :]) .> 0)
                kt = sum(abs.(CM[1:i-1, 1:i-1]) .> 0)

                #= for k in 1:L
                    kt = kt + sum(M[k,:])^α
                end =#

                p = k/kt

                if rand() < p && CM[i, j] != 1 && m_count < m
                    
                    CM[i, j] = cablecount
                    CM[j, i] = -1*cablecount
                    m_count += 1
                    cablecount += 1
                end
            end
        end
    end

    return CM, cablecount - 1
end

function MG_Barabasi_Albert(num_sources; m = 1, α = 0)

    #=
        The Barabasi-Albert model is an algorithm for generating random scale-free networks
        using a preferential attachment mechanism. Several natural and human-made systems,
        including the Internet, the World-wide-web, and some social networks are thought
        to be approximately scale-free and certainly contain few nodes (called hubs) with
        unusually high degree as compared to other nodes of the network.

        The network can begin with an initial connected network of m0 nodes.

        New nodes are added to the network one at a time. Each node is connected to
        m<= m0 existing nodes with a probability that is proportional to the number of
        links that the existing nodes already have.

        In this case m = 1 or m = 2. m0 is equal to 2

        if α is not equal to 1 then we have the more general non-linear preferential
        attachment (NLPA) model. If 0 < α < 1, NLPA is referred to as "sub-linear" and
        the degree distribution of the network tends to a stretched exponential
        distribution. if α > 1, NLPA is referred to as "super-linear" and a small
        number of nodes connect to almost all other nodes in the network. For both
        α < 1 and α > 1, the scale free property of the network is broken in the
        limit of infinite system size.
    =#

    CM = Array{Int64, 2}(undef, num_sources, num_sources)
    CM = fill!(CM, 0)

    cablecount = 1

    if num_sources > 1

        CM[1,2] = 1 #just connecting the first node with the second

        cablecount += 1

        for i in 3:num_sources

            m_count = 0

            while m_count < m

                for j in 1:i-1

                    k = sum(abs.(CM[j, :]) .> 0)
                    kt = sum(abs.(CM[1:i-1, 1:i-1]) .> 0)

                    #= for k in 1:L
                        kt = kt + sum(M[k,:])^α
                    end =#

                    p = k/kt

                    if rand() < p && CM[i, j] != 1 && m_count < m
                        
                        CM[i, j] = cablecount
                        m_count += 1
                        cablecount += 1
                    end
                end
            end
        end
    end

    #= CM = NewCM(CM, size(CM, 1), 2*num_sources) 

    for i in 1:num_sources

        # Add controllable Load

        node1 = i
        node2 = i + num_sources
        CM[node1, node2] = cablecount
        cablecount += 1

        # Add Static Load

        node1 = i
        node2 = i + 2*num_sources
        CM[node1, node2] = cablecount
        cablecount += 1
    end =#

    CM = NewCM(CM, size(CM, 1), num_sources) 

    for i in 1:num_sources

        # Add Static Load

        node1 = i
        node2 = i + num_sources
        CM[node1, node2] = cablecount
        cablecount += 1
    end

    CM = CM - transpose(CM)

    return CM, cablecount - 1
end

function FindpathLengthsFromNode(M, node)

    #=
        Returns for each node2 in the graph the shortest distance from node to node2
        given node, returns the shortest distances from all other nodes to the given
        node. Therefore, we are returning a vector.

        Without the random long edges, the shortest path between i and j will be given
        by hopping in steps of length Z/2 along the shorter of the two arcs around the
        circle; there will be no paths of length longer than L/Z (halfway around the
        circle), and the distribution ρ(l) of path lengths will be constant for
        0 < l < L/Z. When we add shortcuts, we expect that the distribution will be
        shifted to shorter path lengths.

        We can implement a breadth-first traversal of the graph, working outward
        from node in shells. There will be a CurrentShell of nodes whose distance
        will be set to l unless they have already been visited, and nextshell which
        will be considered after the current one is finished (looking sideways
        before forward, i.e. breadth first). The idea is to sweep outward from node,
        measuring the shortest distance to every other node in the network.
    =#

    l = 0 # initialise, the distance from the node to itself is zero
    CurrentShell = [node]
    NextShell = Array{Float64, 1}(undef, 0)
    L = size(M,1)
    distances = Array{Float64, 1}(undef, L)
    distances = fill!(distances, -1.0)

    distances[node] = l

    count = 0

    while length(CurrentShell) != 0

        #=
        For each neighbour of each node in the current shell, if the distance
        to neighbour has not been set, add the node to nextShell and set the
        distance to l + 1
        =#

        for i in eachindex(CurrentShell)

            N = GetNeighbours(M, convert(Int64, round(CurrentShell[i])))

            if length(N) != 0
                for j in eachindex(N)
                    if distances[convert(Int64, round(N[j]))] == -1
                        NextShell = push!(NextShell, N[j])
                        distances[convert(Int64, round(N[j]))] = l + 1
                    end
                end
            end
        end

        CurrentShell = NextShell

        l = l + 1

        #Fail safe
        count = count + 1
        if count > L
            break
        end
    end

    return distances
end

function FindAllPathLengths(M)
    # Generates a list of all lengths (one per pair of nodes).

    L = size(M,1)
    distances = Array{Float64, 1}(undef, 0)
    distances = transpose(FindpathLengthsFromNode(M, 1)[2:L])

    for i in 2:(L-1)
        distances = [distances transpose(FindpathLengthsFromNode(M, i)[(i+1):L])]
    end

    return distances
end

function CCA_dynamics(Net, nodekey; T = 2, κ = 4)

    #= Theory:
        Here we have the dynamics of cyclic cellular automata, which are 
        models of pattern formation in excitable media. Each site in a 
        square lattice has one of κ colours. A cell of colour "nodekey" will
        change its colour to (nodekey + 1)mod(κ) if there are already at least
        T ("threshold") cells of that colour in its neighbourhood, i.e., within
        a distance r ("range") of that cell. Otherwise, the cell keeps its 
        current colour. (In normal excitable media, which have a unique quiescent
        state, the role of the threshold is slightly different). All the cells
        update their colours in parallel.

        CCA have three generic long-run behaviours, depending on the ratio of the
        threshold to the range. At high thresholds, CCA form homogeneous blocks of 
        solid colours, which are completely static ("fixation"). At very low thresholds, 
        the entire lattice eventually oscillates periodically; sometimes rotating spiral
        grow to engulf the entire lattice. With intermediate thresholds, incoherent
        traveling waves form, propagate, collide and disperse; this, metaphorically,
        is "turbulence". With a range of one Moore (box) neighbourhood and κ = 4, the 
        phenomology is as follows: T = 1 and T = 2 are both locally periodic, but T = 2
        produces spiral waves, while T = 1 quenches incoherent local oscillations. T = 3
        leads to meta-stable turbulence - spiral waves can form and entrain the CA, but
        turbulence can persist indefinitely on finite lattices. Fixation occurs with T >= 4.
    =#

    next_nodekey = Array{Float64, 1}(undef, L)

    for j in 1:L

        nn = GetMoore(Net, j, true) #GetNeighbours(Net, j) #
        c = 0

        new_key = (nodekey[j] + 1)%κ
        for k in eachindex(nn)
            
            if nodekey[nn[k]] == new_key
                c = c + 1
            end 
        end

        if c >= T
            next_nodekey[j] = new_key
        else
            next_nodekey[j] = nodekey[j]
        end
    end

    return next_nodekey
end

function initialise_CCA(x0)

    L = length(x0)

    for i in 1:L # defining the initial state "colours" of the nodes
        
        g = 4*rand()
        if g < 1
            x0[i] = 0
        elseif g < 2
            x0[i] = 1
        elseif g < 3
            x0[i] = 2
        elseif g <= 4
            x0[i] = 3
        end
    end

    return x0
end

#_______________________________________________________________________________
# Plotting functions

function Draw_Plots(Deus::ϵ_Machine)

    fig = Figure(backgroundcolor = RGBf(0.98, 0.98, 0.98), resolution=(1600, 800))
    G_heading = fig[1, 1:3] = GridLayout()
    Label(G_heading[1,1], "Statistical Mechanics of Networks", 
            textsize = 30,
            font = "TeX Gyre Heros Bold")

    G_A = fig[2, 1] = GridLayout()
    G_B = fig[2, 2] = GridLayout()
    G_C = fig[2, 3] = GridLayout()
    G_D = fig[3, 2] = GridLayout()

    display(fig)

    #_______________________________________________________________________________   

    fnd = findall(x->x != (0.00), Deus.pv)
    rel_dangs = sum(Deus.pv[Deus.Dang_States] .!= 0.0)
    Ergodicity = 100 - 100*rel_dangs/length(fnd)

    Label(G_A[1,1], 
            textsize = 20, 
            halign = :center,
            valign = :center,
            justification = :left,
            lineheight = 1.2,
            "Digraph Indeterminacy\n"*
            "Ergodicity\n\n"*
            "Hartley Entropy Rate\n"*
            "Kolmogorov-Sinai Entropy Rate\n"*
            "Topological State Complexity\n"*
            "Metric State Complexity\n"*
            "Topological transition Complexity\n"*
            "Metric transition Complexity")

    Label(G_A[1,2], 
    textsize = 20, 
    halign = :center,
    valign = :center,
    justification = :left,
    lineheight = 1.2,
            "Ig\n"*
            "ℸ\n\n"*
            "h\n"*
            "hμ\n"*
            "C\n"*
            "Cμ\n"*
            "Ce\n"*
            "Cμe")
    
    vals = Array{Float64, 1}(undef, 8)
    vals[1] = Deus.IG
    vals[2] = Ergodicity
    vals[3] = Deus.h
    vals[4] = Deus.hμ
    vals[5] = Deus.C
    vals[6] = Deus.Cμ
    vals[7] = Deus.Ce
    vals[8] = Deus.Cμe
    vals = round.(vals, digits = 3)
    str = Array{string, 1}
    str = string.(vals)

    Label(G_A[1,3], 
    textsize = 20, 
    halign = :center,
    valign = :center,
    justification = :left,
    lineheight = 1.2,
            str[1]*"\n"*
            str[2]*"\n\n"*
            str[3]*"\n"*
            str[4]*"\n"*
            str[5]*"\n"*
            str[6]*"\n"*
            str[7]*"\n"*
            str[8])

    Label(G_A[1,4], 
    textsize = 20, 
    halign = :center,
    valign = :center,
    justification = :left,
    lineheight = 1.2,
            "[bits]\n"*
            "[%]\n\n"*
            "[bits/sym]\n"*
            "[bits/sym]\n"*
            "[Turings]\n"*
            "[Turings]\n"*
            "[Turings]\n"*
            "[Turings]")

    pv_ax = Axis(G_A[2, 1:4], 
                    title = "Stationary Probability Distribution", 
                    xlabel = "States", 
                    ylabel = "Probability")

    barplot!(pv_ax, 1:length(fnd), sort(Deus.pv[fnd]), color = :teal, strokecolor = :black, strokewidth = 0)
    lfnd = convert(Int64, ceil(length(fnd)/10))
    minfnd = minimum(fnd)
    maxfnd = maximum(fnd)
    pv_ax.xticks = 1:lfnd:length(fnd)

    rowsize!(G_A, 1, Fixed(300))
    rowsize!(G_A, 2, Fixed(300))

    colsize!(G_A, 1, Fixed(300))
    colsize!(G_A, 2, Fixed(50))
    colsize!(G_A, 3, Fixed(50))
    colsize!(G_A, 4, Fixed(125))

    #_______________________________________________________________________________

    hα_ax = Axis(G_B[1, 1], 
                    title = "Rényi Entropy Rate", 
                    xlabel = "α", 
                    ylabel = "hα [bits/symbol]")
    lines!(hα_ax, Deus.α, Deus.hα, color = :green4)
    hα_ax.xticks = 0:0.1:1.0

    Cα_ax = Axis(G_B[2, 1], 
                    title = "State Complexity of Order - α", 
                    xlabel = "α", 
                    ylabel = "Cα [Turings]")
    lines!(Cα_ax, Deus.α, Deus.Cα, color = :dodgerblue)
    Cα_ax.xticks = 0:0.1:1.0

    Cαe_ax = Axis(G_B[3, 1], 
                    title = "Transition Complexity of Order - α", 
                    xlabel = "α", 
                    ylabel = "Cαe [Turings]")
    lines!(Cαe_ax, Deus.α, Deus.Cαe, color = :navyblue)
    Cαe_ax.xticks = 0:0.1:1.0

    colsize!(G_B, 1, Fixed(400))
    rowsize!(G_B, 1, Fixed(150))
    rowsize!(G_B, 2, Fixed(150))
    rowsize!(G_B, 3, Fixed(150))
    
    #_______________________________________________________________________________

    Zα_ax = Axis(G_C[1, 1], 
                    title = "Cylinder Partition Function", 
                    xlabel = "α", 
                    ylabel = "Ζα [bits]")
    lines!(Zα_ax, Deus.α, Deus.Zα, color = :purple)
    Zα_ax.xticks = 0:0.1:1.0

    Hα_ax = Axis(G_C[2, 1], 
                    title = "Cylinder Rényi Entropy", 
                    xlabel = "α", 
                    ylabel = "Hα [bits]")
    lines!(Hα_ax, Deus.α, Deus.Hα, color = :turquoise3)
    Hα_ax.xticks = 0:0.1:1.0

    Cμ_t_ax = Axis(G_C[3, 1], 
                    title = "Knowledge Relaxation", 
                    xlabel = "Time [seconds]", 
                    ylabel = "Cμ [bits]")
    lines!(Cμ_t_ax, Deus.t_m, Deus.Cμ_t, color = :red)
    minfnd = minimum(Deus.t_m)
    maxfnd = maximum(Deus.t_m)
    interval = round((maxfnd-minfnd)/5, digits = 3)
    Cμ_t_ax.xticks = minfnd:(maxfnd-minfnd)/5:maxfnd

    colsize!(G_C, 1, Fixed(300))

    #_______________________________________________________________________________
    
    return nothing
end

function All_Plots(Deus::ϵ_Machine)

    fnd = findall(x->x != (0.00), Deus.pv)
    rel_dangs = sum(Deus.pv[Deus.Dang_States] .!= 0.0)
    Ergodicity = 100 - 100*rel_dangs/length(fnd)

    minfnd = minimum(fnd)
    maxfnd = maximum(fnd)
    lfnd = convert(Int64, ceil(length(fnd)/10))

    p1 = bar(1:length(fnd), sort(Deus.pv[fnd]), 
        xticks = 1:lfnd:length(fnd),
        title = "Stationary Probability Distribution", 
        xlabel = "States", 
        ylabel = "Probability",
        linecolor = :turquoise3,
        color = :turquoise3,
        legend = false)
    display(p1)

    p2 = plot(Deus.α, Deus.hα,
            title = "Rényi Entropy Rate", 
            xlabel = "α", 
            ylabel = "hα [bits/symbol]",
            color = :green4,
            legend = false,
            grid = true,
            gridlinewidth = 2)
    display(p2)

    p3 = plot(Deus.α, Deus.Cα,
            title = "State Complexity of Order - α", 
            xlabel = "α", 
            ylabel = "Cα [Turings]",
            color = :dodgerblue,
            legend = false,
            grid = true,
            gridlinewidth = 2)
    display(p3)

    p4 = plot(Deus.α, Deus.Cαe,
            title = "Transition Complexity of Order - α", 
            xlabel = "α", 
            ylabel = "Cαe [Turings]",
            color = :dodgerblue,
            legend = false,
            grid = true,
            gridlinewidth = 2)
    display(p4)
        
    return nothing
end

function Draw_Graph(Net, nodekey, N; run = 0)

    Graph_fig = Figure(backgroundcolor = RGBf(0.98, 0.98, 0.98), resolution=(800, 800))

    Ts = SimpleGraph(Net) # create graph object from Connecitivity Matrix

    Graph_fig[1, 1] = graph_ax = Axis(Graph_fig)
    
    nodecolours = Array{Symbol, 1}(undef, L)
    nodecolours = fill!(nodecolours, :blue)

    #set_theme!(resolution = (800, 800))
    p = graphplot!(graph_ax, Ts;
            edge_color = [RGBAf(0,0,0,0) for i in 1:ne(Ts)], #RGBAf(0,0,0,0)
            node_size = 20,
            node_marker = :rect,
            fontsize = 1,
            node_color = nodecolours,
            #curve_distance =-.5,
            #curve_distance_usage = true
            )
    
    #Circular_Layout(p, graph_ax, L)
    Plane_Layout(p, graph_ax, L)
    #p.layout = Spring(Ptype = Float32)
    hidedecorations!(graph_ax)
    #hidespines!(ax)
    #ax.aspect = DataAspect()

    display(Graph_fig)

    #_______________________________________________________________________________
    # Graph Animation

    start = N
    if run == 1
        start = 1
    end
    for t in start:N

        for n in 1:L

            if nodekey[n, t] == 0
                nodecolours[n] = :green4   
            elseif nodekey[n, t] == 1
                nodecolours[n] = :dodgerblue
            elseif nodekey[n, t] == 2
                nodecolours[n] = :royalblue4
            elseif nodekey[n, t] == 3
                nodecolours[n] = :navyblue
            else
                nodecolours[n, t] = :blue
            end
        end

        p.node_color[][:] = nodecolours[:]
        p.node_color[] = p.node_color[]

        if run == 1
            sleep(0.05)
        end
    end
       
    return nothing
end




 



    



