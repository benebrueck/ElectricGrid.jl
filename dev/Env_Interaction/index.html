<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Interaction with the Environment · Julia Electric Grid</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="Julia Electric Grid logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Julia Electric Grid</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><a class="tocitem" href="../NodeConstructor_Theory/">The Nodeconstructor - Theory</a></li><li><a class="tocitem" href="../NodeConstructor_Application/">The Nodeconstructor - Application</a></li><li><a class="tocitem" href="../classical/">Set up Classical Controllers</a></li><li><a class="tocitem" href="../dev/">Contributing</a></li></ul></li><li><span class="tocitem">Environment</span><ul><li><a class="tocitem" href="../Env_Create/">Configuring the Environment</a></li><li class="is-active"><a class="tocitem" href>Interaction with the Environment</a><ul class="internal"><li><a class="tocitem" href="#Basic-interaction-with-the-environment"><span>Basic interaction with the environment</span></a></li><li><a class="tocitem" href="#The-MultiController"><span>The MultiController</span></a></li><li><a class="tocitem" href="#Data-Hook-(how-to-tap-the-simulated-data-stream)"><span>Data-Hook (how to tap the simulated data stream)</span></a></li><li><a class="tocitem" href="#AC-grid-example"><span>AC grid example</span></a></li></ul></li></ul></li><li><span class="tocitem">Reinforcement Learning</span><ul><li><a class="tocitem" href="../RL_Single_Agent/">Reinforcement Learning using JEG</a></li><li><a class="tocitem" href="../RL_Classical_Controllers_Merge/">Multicontroller</a></li><li><a class="tocitem" href="../RL_Complex/">Reinforcement Learning in Larger Grids </a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Environment</a></li><li class="is-active"><a href>Interaction with the Environment</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Interaction with the Environment</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/upb-lea/JuliaElectricGrid.jl/blob/main/docs/src/Env_Interaction.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Interaction-with-the-ElectricGridEnv"><a class="docs-heading-anchor" href="#Interaction-with-the-ElectricGridEnv">Interaction with the ElectricGridEnv</a><a id="Interaction-with-the-ElectricGridEnv-1"></a><a class="docs-heading-anchor-permalink" href="#Interaction-with-the-ElectricGridEnv" title="Permalink"></a></h1><p>In the previous <a href="https://upb-lea.github.io/JuliaElectricGrid.jl/dev/Env_Create/">section</a> it was shown how to set up an environment. This section is intended to show how to interact with the environment and, above all, how to extract, store the data and covers the following topics:</p><ul><li><h3>Apply actions,</h3></li><li><h3>Data hook (how to tap the simulated data stream),</h3></li><li><h3>AC grid example.</h3></li></ul><p>The interactive content related to the section described here can be found in the form of a notebook <a href="https://github.com/upb-lea/JuliaElectricGrid.jl/blob/main/examples/notebooks/Env_Interaction_DEMO.ipynb">here</a>.</p><h2 id="Basic-interaction-with-the-environment"><a class="docs-heading-anchor" href="#Basic-interaction-with-the-environment">Basic interaction with the environment</a><a id="Basic-interaction-with-the-environment-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-interaction-with-the-environment" title="Permalink"></a></h2><p>The dynamic behaviour of the environment is simulated using linear state-space models.  It interacts step-wise with the agent/controller like shown in the figure below.  Based on the input/action <code>u</code> at timestep <code>k</code> the state <code>x</code> is calculated.</p><p><img src="../assets/RL_env.png" alt/></p><p>Based on that action <code>u_k</code> and the internal state-space model, the system response is evolved for one timestep and the new states <code>x_k+1</code> are calulated. The state-space model is defined depending on the electric components - for more information about the ordinary differential equations behind,... see <a href="https://upb-lea.github.io/JuliaElectricGrid.jl/dev/NodeConstructor_Theory/">The Nodecontructor - Theory</a>.</p><p>In the following will be described how to use the described parameter dict to define a simple, small example <code>env</code> to learn how to interact with it. This environment consists of a single phase electrical power grid with 1 source and 1 load as shown in the figure below. For reasons of clarity, only phase a is shown:</p><p><img src="../assets/S1_L1_all.png" alt/></p><p>To get an <code>env</code> consisting of that specific setting with the correct filter type and load, the parameter dict is defined in beforehand and handed over to the <code>env</code>. Instead of <code>num_sorces</code> and <code>num_loads</code>, now the parameter dict and the connection matrix <code>CM</code> is used which defines if there is a connection between two nodes (e.g., source &lt;-&gt; load) or not.  If there is a connetion the corresponding matrix entry is not zero, if there is no connection the entry is <code>0</code>. For more information about the CM matrix see <a href="https://upb-lea.github.io/JuliaElectricGrid.jl/dev/NodeConstructor_Application/">The Nodecontructor - Application</a>.</p><p>To create a usefull example we first calulate a load which fits in case of power rating to the power of the source. Therefore, the function <code>ParallelLoadImpedance()</code> provied by the <code>JEG</code> package is used which calulates the passive parameters for a load for specified apparant power.</p><pre><code class="language-julia hljs">using JEG

S_source = 200e3

S_load = 150e3
pf_load = 1
v_rms = 230
R_load, L_load, X, Z = ParallelLoadImpedance(S_load, pf_load, v_rms)</code></pre><p>Then we use these values during definition of the <code>env</code> in the parameter dict.</p><pre><code class="language-julia hljs">CM = [0. 1.
    -1. 0.]

parameters = Dict{Any, Any}(
        &quot;source&quot; =&gt; Any[
                        Dict{Any, Any}(&quot;pwr&quot; =&gt; S_source, &quot;control_type&quot; =&gt; &quot;classic&quot;, &quot;mode&quot; =&gt; &quot;Step&quot;, &quot;fltr&quot; =&gt; &quot;LC&quot;),
                        ],
        &quot;load&quot;   =&gt; Any[
                        Dict{Any, Any}(&quot;impedance&quot; =&gt; &quot;R&quot;, &quot;R&quot; =&gt; R_load),
                        ],
        &quot;cable&quot;   =&gt; Any[
                        Dict{Any, Any}(&quot;R&quot; =&gt; 1e-3, &quot;L&quot; =&gt; 1e-4, &quot;C&quot; =&gt; 1e-4, &quot;i_limit&quot; =&gt; 1e4, &quot;v_limit&quot; =&gt; 1e4,),
                        ],
        &quot;grid&quot; =&gt; Dict{Any, Any}(&quot;fs&quot;=&gt;1e4, &quot;phase&quot;=&gt;3, &quot;v_rms&quot;=&gt;230, &quot;f_grid&quot; =&gt; 50, &quot;ramp_end&quot;=&gt;0.0)
    )


env = ElectricGridEnv(CM = CM, parameters = parameters)

env.state_ids[1:5]</code></pre><pre><code class="nohighlight hljs">5-element Vector{String}:
 &quot;source1_i_L1_a&quot;
 &quot;source1_v_C_filt_a&quot;
 &quot;source1_v_C_cables_a&quot;
 &quot;cable1_i_L_a&quot;
 &quot;load1_v_C_total_a&quot;</code></pre><p>As can be seen, the five states marked in the equivalent circuit diagram in the figure above can be found via the <code>env.state_ids</code> (<code>_a</code> marking the phase is not displayed in the figure). Analog, the <code>action_ids</code> can be found which consists of 3 entries, one per phase for the defined source.</p><pre><code class="language-julia hljs">env.action_ids</code></pre><pre><code class="nohighlight hljs">3-element Vector{String}:
 &quot;source1_u_a&quot;
 &quot;source1_u_b&quot;
 &quot;source1_u_c&quot;</code></pre><p>To interact with the env, the function <code>env(action)</code> can be called:</p><pre><code class="language-julia hljs">env([0.2, 0.2, 0.2])</code></pre><p>Here, the source gets an action of <code>0.2</code> to all three phases. This interaction can be done in a loop while the state is logged during this process:</p><pre><code class="language-julia hljs"># run 3 steps
reset!(env)
for _ in 1:3
    env([0.2, 0.2, 0.2])
end

env.state[1:5]  # print states after 3 steps</code></pre><pre><code class="nohighlight hljs">5-element Vector{Float64}:
 0.06548135420290145
 0.013847803005637331
 0.01692657505439206
 0.0012287741401077802
 0.020386045162012048</code></pre><p>The <code>control_type</code> of the source is chosen as classic in <code>mode =&gt; Step</code>. That means it is open loop and a step is given to the input resulting in an input voltage of <code>env.nc.parameters[&quot;grid&quot;][&quot;v_rms&quot;]</code>. For more information about possible control modes, see <code>ClassicalController_Notebook</code> or <a href="https://upb-lea.github.io/JuliaElectricGrid.jl/dev/classical/">documentation</a>.</p><h2 id="The-MultiController"><a class="docs-heading-anchor" href="#The-MultiController">The MultiController</a><a id="The-MultiController-1"></a><a class="docs-heading-anchor-permalink" href="#The-MultiController" title="Permalink"></a></h2><p>The <code>JEG</code> toolbox provides a more enhanced method to run an experiment with a specific number of steps and even more episodes. It is based in the <code>run</code> command provided by the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/src/ReinforcementLearningCore/src/core/run.jl">ReinforcementLeaning.jl/run</a> toolbox and, therefore, can be used to <code>learn</code> a control task or to <code>simulate</code> a fixed setup.</p><p>First we take a look onto simulating the behaviour of an env without learning. Therefore, the <code>Simulate()</code> function from the <code>MultiController</code> is used.  This acts as a configuration tool to define the corresponding classic controllers and RL agents. Additionally, it links inputs to the <code>env.state_ids</code> and outputs to the <code>env.action_ids</code>.</p><p>In our example, we only apply classic controllers to learn how the tool works (learning with RL agents is investigated in <code>RL_Single_Agent_DEMO.ipynb</code>).  The interaction of the different components (env, agent, classic controllers, wrapper,...) is shown in the diagram below.</p><p><img src="./assets/DareOverview.png &quot;&quot;" alt/></p><p>The depicted multiagent ensures that the states and actions for every source are exchanged with the correct agent or classic controller. This is depending on the <code>control_types</code> and <code>mode</code>s. Therefore, the <code>SetupAgents(env)</code> method returns a <code>Controller</code> of type <code>MultiController</code>.</p><p>Then, the experiment is executed for (default) one episode. The length of the episode is defined depending on the parameter <code>env.maxsteps</code> (default = 500), which alternatively can be defined via <code>env.t_end</code> and <code>env.ts</code>.  Inside <code>Simulate()</code> the run command (RL.base) executes 500 steps applying the defined controllers in the env. Every step the control inputs are calculated based on the corresponding states and the env is evolved for <code>ts</code>. Then, the new state is given to the controller/agent again. </p><p>The 500 steps only gets executed if none of the defined voltage and current limits of the states are exceeded (which would be equivalent to a system crash). In that case, a flag in the env is set (<code>env.done = true</code>) which terminates the episode.</p><p>To investigate the env &lt;-&gt; agent interaction (without learning), the <code>Simulate()</code> method can be used.</p><pre><code class="language-julia hljs">Controller = SetupAgents(env);
hook = Simulate(Controller, env);
hook.df</code></pre><p>If not defined in beforehand and handed over to the <code>Simulate()</code> methode, it returns a default <code>hook</code>. </p><h2 id="Data-Hook-(how-to-tap-the-simulated-data-stream)"><a class="docs-heading-anchor" href="#Data-Hook-(how-to-tap-the-simulated-data-stream)">Data-Hook (how to tap the simulated data stream)</a><a id="Data-Hook-(how-to-tap-the-simulated-data-stream)-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Hook-(how-to-tap-the-simulated-data-stream)" title="Permalink"></a></h2><p>To collect the data - so how the states evolved over time by the chosen actions - hooks (<a href="https://juliareinforcementlearning.org/docs/How_to_use_hooks/">ReinforcmentLearning.jl/data-hooks</a>) are used which will be executed at different stages of the experiment. Via theses hooks the data to be collected is stored in a <code>DataFrame</code> labeled via the state and action IDs. This all happens using the <code>DataHook</code> provided by the <code>JEG</code> package.</p><p>This <code>DataFrame</code> can be seen in the output of the upper cell. It contains 500 rows - one for each simulated step. In the different colons the information about the simulation is stored, e.g., the number of the episode, time, states, actions... By default a <code>DefaultDataHook</code> is generated which collects all available information about the states of the <code>env</code>. </p><p>If we are interested how the state of the current through the filter inductor of source 1 (first state in the env <code>source1_i_L1_a</code>) evolves over time, we can grap the data like follows:</p><pre><code class="language-julia hljs">id = env.state_ids[1] # get first state_id
println(id)
hook.df[:, id]</code></pre><pre><code class="nohighlight hljs">source1_i_L1_a



500-element Vector{Float64}:
   0.0
   0.0
  64.38793241613949
 118.5118638524073
 159.6623242035541
 188.4161060129909
 206.7479466095761
 216.9637166049256
 221.29328943565338
 221.71322649159114
   ⋮
 204.02614278089519
 204.02614278089519
 204.02614278089519
 204.02614278089519
 204.02614278089519
 204.02614278089519
 204.02614278089519
 204.02614278089519
 204.02614278089519</code></pre><p>If we only want to log specific states during the experiment, a <code>DataHook</code> can be defined in beforehand. It only has to be defined which states and actions should be logged during initialisation. Then the <code>hook</code> has to be handed over to the <code>Simulate()</code> (or <code>Learn()</code>) command.</p><p>In the following, we will collect only the state <code>&quot;source1_i_L1_a&quot;</code> and the first action <code>&quot;source1_u_a&quot;</code>. Alternatively, for example the number of source can be used to collect all the data for a specific source (compare out-commented example code). A lot of other flags can be defined to collect more measurements (RMS, reward, powers,...). For more details we refer to the documentation and the other example notebooks.</p><pre><code class="language-julia hljs">hook = DataHook(collect_state_ids = [&quot;source1_i_L1_a&quot;],
                collect_action_ids = [&quot;source1_u_a&quot;]
                #collect_sources  = [1]  # alternative collect all data of source 1 
                );

Simulate(Controller, env, hook=hook);

hook.df</code></pre><p>&lt;div class=&quot;data-frame&quot;&gt;&lt;p&gt;500 rows × 10 columns (omitted printing of 4 columns)&lt;/p&gt;&lt;table class=&quot;data-frame&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;episode&lt;/th&gt;&lt;th&gt;time&lt;/th&gt;&lt;th&gt;source1<em>i</em>L1<em>a&lt;/th&gt;&lt;th&gt;source1</em>v<em>L1</em>a&lt;/th&gt;&lt;th&gt;action&lt;/th&gt;&lt;th&gt;next<em>state</em>source1<em>i</em>L1_a&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th title=&quot;Int64&quot;&gt;Int64&lt;/th&gt;&lt;th title=&quot;Float32&quot;&gt;Float32&lt;/th&gt;&lt;th title=&quot;Float64&quot;&gt;Float64&lt;/th&gt;&lt;th title=&quot;Float64&quot;&gt;Float64&lt;/th&gt;&lt;th title=&quot;Vector{Union{Nothing, Float64}}&quot;&gt;Array…&lt;/th&gt;&lt;th title=&quot;Float64&quot;&gt;Float64&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;1&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;2&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0001&lt;/td&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;td&gt;0.0&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;64.3879&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;3&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0002&lt;/td&gt;&lt;td&gt;64.3879&lt;/td&gt;&lt;td&gt;205.206&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;118.512&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;4&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0003&lt;/td&gt;&lt;td&gt;118.512&lt;/td&gt;&lt;td&gt;163.059&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;159.662&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;5&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0004&lt;/td&gt;&lt;td&gt;159.662&lt;/td&gt;&lt;td&gt;118.471&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;188.416&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;6&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0005&lt;/td&gt;&lt;td&gt;188.416&lt;/td&gt;&lt;td&gt;79.1171&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;206.748&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;7&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0006&lt;/td&gt;&lt;td&gt;206.748&lt;/td&gt;&lt;td&gt;47.433&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;216.964&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;8&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0007&lt;/td&gt;&lt;td&gt;216.964&lt;/td&gt;&lt;td&gt;23.6258&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;221.293&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;9&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0008&lt;/td&gt;&lt;td&gt;221.293&lt;/td&gt;&lt;td&gt;7.07961&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;221.713&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;10&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0009&lt;/td&gt;&lt;td&gt;221.713&lt;/td&gt;&lt;td&gt;-3.29035&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;219.837&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;11&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.001&lt;/td&gt;&lt;td&gt;219.837&lt;/td&gt;&lt;td&gt;-8.84257&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;216.869&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;12&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0011&lt;/td&gt;&lt;td&gt;216.869&lt;/td&gt;&lt;td&gt;-10.9624&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;213.633&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;13&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0012&lt;/td&gt;&lt;td&gt;213.633&lt;/td&gt;&lt;td&gt;-10.866&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;210.628&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;14&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0013&lt;/td&gt;&lt;td&gt;210.628&lt;/td&gt;&lt;td&gt;-9.51317&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;208.116&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;15&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0014&lt;/td&gt;&lt;td&gt;208.116&lt;/td&gt;&lt;td&gt;-7.60108&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;206.185&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;16&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0015&lt;/td&gt;&lt;td&gt;206.185&lt;/td&gt;&lt;td&gt;-5.59584&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;204.82&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;17&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0016&lt;/td&gt;&lt;td&gt;204.82&lt;/td&gt;&lt;td&gt;-3.77656&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;203.942&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;18&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0017&lt;/td&gt;&lt;td&gt;203.942&lt;/td&gt;&lt;td&gt;-2.28148&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;203.448&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;19&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0018&lt;/td&gt;&lt;td&gt;203.448&lt;/td&gt;&lt;td&gt;-1.15116&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;203.234&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;20&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0019&lt;/td&gt;&lt;td&gt;203.234&lt;/td&gt;&lt;td&gt;-0.365185&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;203.206&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;21&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.002&lt;/td&gt;&lt;td&gt;203.206&lt;/td&gt;&lt;td&gt;0.129335&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;203.288&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;22&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0021&lt;/td&gt;&lt;td&gt;203.288&lt;/td&gt;&lt;td&gt;0.397567&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;203.423&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;23&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0022&lt;/td&gt;&lt;td&gt;203.423&lt;/td&gt;&lt;td&gt;0.503928&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;203.573&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;24&lt;/th&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0.0023&lt;/td&gt;&lt;td&gt;203.573&lt;/td&gt;&lt;td&gt;0.504717&lt;/td&gt;&lt;td&gt;[0.575, 0.575, 0.575]&lt;/td&gt;&lt;td&gt;203.713&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;&amp;vellip;&lt;/th&gt;&lt;td&gt;&amp;vellip;&lt;/td&gt;&lt;td&gt;&amp;vellip;&lt;/td&gt;&lt;td&gt;&amp;vellip;&lt;/td&gt;&lt;td&gt;&amp;vellip;&lt;/td&gt;&lt;td&gt;&amp;vellip;&lt;/td&gt;&lt;td&gt;&amp;vellip;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;</p><p>Investigating the <code>DataFrame</code> we can see, that even more states then planned are logged. The <code>JEG</code> framework stores all information available with regards to the state we have chosen. Here, additionally the voltage across the inductor and the state after the action is applied is stored as well.  For more detailed information, see <code>API-Doku -&gt; DataHook</code>.</p><p>After the experiment, the <code>RenderHookResults()</code> function provided by the <code>JEG</code> package can be used to show the results of the simulated episode. States and actions which are logged with the <code>DataHook</code> can be selected to be plotted:</p><pre><code class="language-julia hljs">RenderHookResults(hook = hook, 
                    episode = 1,
                    states_to_plot  = [&quot;source1_i_L1_a&quot;],  
                    actions_to_plot = [&quot;source1_u_a&quot;])</code></pre><p><img src="./assets/env_interaction_DC.png &quot;&quot;" alt/></p><p>As can be seen, the state <code>&quot;source1_i_L1_a&quot;</code> (belonging to the left y-axis) and the action <code>&quot;source1_u_a&quot;</code> (right y-axis) are plotted over the 500 timesteps while one timestep takes <code>ts = 1e-4</code> s resulting in an episode time of 0.05 s. Here, the states and actions plotted/collected are not normalised.</p><p>Above the <code>control_type</code> of the source is chosen as classic in <code>mode =&gt; Step</code>.  The implements an open-loop circuit. As action a step is given to the inputs resulting in an input voltage of <code>env.nc.parameters[&quot;grid&quot;][&quot;v_rms&quot;]</code>.  For more information about possible control modes, see <code>ClassicalController_Notebook</code> or <a href="https://upb-lea.github.io/JuliaElectricGrid.jl/dev/classical/">documentation</a>.</p><h2 id="AC-grid-example"><a class="docs-heading-anchor" href="#AC-grid-example">AC grid example</a><a id="AC-grid-example-1"></a><a class="docs-heading-anchor-permalink" href="#AC-grid-example" title="Permalink"></a></h2><p>To run an AC grid where the source creates a sinusoidal voltage with the frequency of <code>env.nc.parameters[&quot;grid&quot;][&quot;f_grid&quot;]</code> just the control mode has to be changed from <code>Step</code> to <code>Swing</code>.</p><pre><code class="language-julia hljs">CM = [0. 1.
    -1. 0.]


parameters = Dict{Any, Any}(
        &quot;source&quot; =&gt; Any[
                        Dict{Any, Any}(&quot;pwr&quot; =&gt; S_source, &quot;control_type&quot; =&gt; &quot;classic&quot;, &quot;mode&quot; =&gt; &quot;Swing&quot;, &quot;fltr&quot; =&gt; &quot;LC&quot;, &quot;i_limit&quot; =&gt; 1e4, &quot;v_limit&quot; =&gt; 1e4,),
                        ],
        &quot;load&quot;   =&gt; Any[
                        Dict{Any, Any}(&quot;impedance&quot; =&gt; &quot;R&quot;, &quot;R&quot; =&gt; R_load)
                        ],
        &quot;cable&quot;   =&gt; Any[
                        Dict{Any, Any}(&quot;R&quot; =&gt; 1e-3, &quot;L&quot; =&gt; 1e-4, &quot;C&quot; =&gt; 1e-4),
                        ],
        &quot;grid&quot; =&gt; Dict{Any, Any}(&quot;fs&quot;=&gt;1e4, &quot;phase&quot;=&gt;3, &quot;v_rms&quot;=&gt;230, &quot;f_grid&quot; =&gt; 50, &quot;ramp_end&quot;=&gt;0.0)
    )

env = ElectricGridEnv(CM = CM, parameters = parameters)

states_to_plot = [&quot;source1_v_C_filt_a&quot;, &quot;source1_v_C_filt_b&quot;, &quot;source1_v_C_filt_c&quot;]
action_to_plot = [&quot;source1_u_a&quot;]

hook = DataHook(collect_state_ids = states_to_plot)

Controller = SetupAgents(env)
Simulate(Controller, env, hook=hook)

RenderHookResults(hook = hook,
                  states_to_plot  = states_to_plot)</code></pre><p><img src="./assets/env_interaction_AC.png &quot;&quot;" alt/></p><p>Based on this introduction, more enhanced environments can be created consisting of more sources and loads connected to each other, e.g.,</p><ul><li>controlled by classic grid controllers to share the power based on the droop concept –&gt; See: <code>Classic_Controllers_Notebooks</code>,</li><li>train RL agents to fullfill current control for single sources –&gt; See: <code>RL_Single_Agent_DEMO.ipynb</code>,</li><li>train RL agents to interact in a larger power grid with classically controlled sources –&gt; See: <code>RL_Classical_Controllers_Merge_DEMO.ipynb</code>.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Env_Create/">« Configuring the Environment</a><a class="docs-footer-nextpage" href="../RL_Single_Agent/">Reinforcement Learning using JEG »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Thursday 30 March 2023 19:17">Thursday 30 March 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
