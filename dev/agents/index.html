<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Set up Agents · Julia Electric Grid</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="Julia Electric Grid logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Julia Electric Grid</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><a class="tocitem" href="../environment/">Configuring the Environment</a></li><li><a class="tocitem" href="../classical/">Set up Classical Controllers</a></li><li class="is-active"><a class="tocitem" href>Set up Agents</a><ul class="internal"><li><a class="tocitem" href="#Train-an-RL-agent"><span>Train an RL agent</span></a></li></ul></li><li><a class="tocitem" href="../nodeconstructor/">The Nodeconstructor</a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Guide</a></li><li class="is-active"><a href>Set up Agents</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Set up Agents</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/upb-lea/JuliaElectricGrid.jl/blob/main/docs/src/agents.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h2 id="Train-an-RL-agent"><a class="docs-heading-anchor" href="#Train-an-RL-agent">Train an RL agent</a><a id="Train-an-RL-agent-1"></a><a class="docs-heading-anchor-permalink" href="#Train-an-RL-agent" title="Permalink"></a></h2><p>This notebook will focus the following topics:</p><ul><li>define reward function,</li><li>define featurize function,</li><li>training a single RL agent.</li></ul><p>In this notebook a reinforcement learning agent is trained to control the current flowing through an inductor. It will be shown for an easy case how the agent can learn and be applied to an electrical power grid simulated with de JEG package.</p><p>The use case is shown in the figure below. This environment consists of a single phase electrical power grid with 1 source and 1 load connected via a cable.</p><p><img src="../assets/RL_single_agent.png" alt/></p><p>First we define the environment with the configuration shown in the figure.  For more information on how to setup an environment see <code>Env_Create_DEMO.ipynb</code>.</p><p><code>RL</code> is selected as <code>control_type</code> for the source (<code>parameters[&quot;source&quot;][1][&quot;control_type&quot;]</code>). Initially, any key can be used as the <code>mode</code>. Here, we choose the name <code>my_ddpg</code>.  This key is then used to link an agent to the source and its corresponding state and action ids. Based on these indices, the state that will be provided to the agent as well as the actions the agent outputs are passed to the appropriate places with the help of a <code>MultiController</code>. For further details please refer to Userguide -&gt; <code>MultiController</code>.</p><pre><code class="language-julia hljs">using JEG
using ReinforcementLearning</code></pre><pre><code class="nohighlight hljs">Failed to start the Kernel. 


Unable to start Kernel &#39;Julia 1.8.2&#39; due to connection timeout. 


View Jupyter &lt;a href=&#39;command:jupyter.viewOutput&#39;&gt;log&lt;/a&gt; for further details.</code></pre><pre><code class="language-julia hljs"># calculate passive load for wanted setting / power rating
R_load, L_load, X, Z = ParallelLoadImpedance(100e3, 1, 230)

# define grid using CM
CM = [0. 1.
    -1. 0.]

# Set parameters accoring graphic above
parameters = Dict{Any, Any}(
    &quot;source&quot; =&gt; Any[
                    Dict{Any, Any}(&quot;pwr&quot; =&gt; 200e3, &quot;control_type&quot; =&gt; &quot;RL&quot;, &quot;mode&quot; =&gt; &quot;my_ddpg&quot;, &quot;fltr&quot; =&gt; &quot;L&quot;),
                    ],
    &quot;load&quot;   =&gt; Any[
                    Dict{Any, Any}(&quot;impedance&quot; =&gt; &quot;R&quot;, &quot;R&quot; =&gt; R_load, &quot;v_limit&quot;=&gt;1e4, &quot;i_limit&quot;=&gt;1e4),
                    ],
    &quot;grid&quot; =&gt; Dict{Any, Any}(&quot;phase&quot; =&gt; 1)
)</code></pre><p>To teach the agent that it should control the current in a certain way it needs information about which value the current shoud be (reference value) (-&gt;<code>featurize</code>) and how good the state is which was reached using the chosen action (-&gt; <code>reward</code>).</p><p>Therefore, the reference value has to be defined.  Here we will use a constant value to keep the example simple. But since the the <code>reference(t)</code> function take the simulation time as argument, more complex, time dependent signals could be defined.</p><pre><code class="language-julia hljs">function reference(t)
    return 1
end</code></pre><p>Afterwards the <code>featurize()</code> function, which gives the user the opportunity to modify a state before it gets passed to the agent, is defined.</p><p>It takes three arguments:</p><ul><li><code>state</code> contains all the state values that correspond to the source controlled by agent with key <code>name</code></li><li><code>env</code> references the environment</li><li><code>name</code> contains the key of the agent</li></ul><p>The signal generated by the <code>reference</code> function is then added to the state for the agent <code>my_ddpg</code>. This will help the agent to learn because later we will define a reward that has maximum value if the measured current fits the reference value. The reference value has to be normalized in an appropirate way that it fits to the range of the normalized states.</p><p>Additionally more signals could be added here to enhance the learning process.</p><p>As stated before, <code>state</code> already contains all state values of the source the agent with key <code>name</code> should control. However, the environment maintains a lot more states than that. Through <code>featurize</code> we could expose them to the agent but we refrain from that here since we want to simulate a scenario where the the source the agent controls is far away (e.g. 1km) from the load its supplying.  In cases like this it&#39;s common that the agent has no knowlegde about states of the load since no communication and measurements exchange between source and load is assumed.</p><p>In onther examples the electrical power grid consits of multiple sources and loads. The other sources are controlled by other agents or classic controllers. In that case, typically every controller / agent has knowlegde of the states of the source it controls but not about the states another agent/controller controls. (For more information see <code>MultiController</code> and <code>inner_featurize</code> of the <code>env</code>.)</p><pre><code class="language-julia hljs">featurize_ddpg = function(state, env, name)
    if name == &quot;my_ddpg&quot;
        norm_ref = env.nc.parameters[&quot;source&quot;][1][&quot;i_limit&quot;]
        state = vcat(state, reference(env.t)/norm_ref)
    end
end</code></pre><p>Before defining the environment, the <code>reward()</code> function has to be defined. It provides a feedback to the agent on how good the chosen action was. First, the state to be controlled is taken from the current environment state values. Since the states are normalized by the limits the electrical components can handle, a value greater than <code>1</code> means that the state limit is exceeded typically leading to a system crash. Therefore, first it is checked if the measured state is greater than <code>1</code>. In that case a punishment is returned which, here, is chosen to be <code>r = -1</code>.</p><p>In case the controlled state is within the valid state space, the reward is caculated based on the error between the wanted reference value and the measured state value.  If these values are the same, meaning the agent perfectly fullfills the control task, a reward of <code>r = 1</code> is returned to the agent. ( -&gt; r <span>$\in$</span> [-1, 1]). If the measured value differs from the reference, the error - based on the root-mean square error (RMSE) in this example - is substracted from the maximal reward: <code>r = 1 - RMSE</code>:</p><p class="math-container">\[r = 1 - \sqrt{\frac{|i_\mathrm{L,ref} - i_\mathrm{L1}|}{2}}\]</p><p>To keep the reward in the wanted range, the current difference is devided by 2. (E.g., in worst case, if a reference value equal to the corresponding current limit is chosen <span>$i_\mathrm{L,ref} = i_\mathrm{lim}$</span> and the measured current is the negative current limit <span>$i_\mathrm{L1} = -i_\mathrm{lim}$</span> more the 1 would be substracted without this normaization).</p><pre><code class="language-julia hljs">function reward_function(env, name = nothing)
    if name == &quot;my_ddpg&quot;
        index_1 = findfirst(x -&gt; x == &quot;source1_i_L1&quot;, env.state_ids)
        state_to_control = env.state[index_1]

        if any(abs.(state_to_control).&gt;1)
            return -1
        else

            refs = reference(env.t)
            norm_ref = env.nc.parameters[&quot;source&quot;][1][&quot;i_limit&quot;]          
            r = 1-((abs.(refs/norm_ref - state_to_control)/2).^0.5)
            return r 
        end
    end
end</code></pre><p>Then, the defined parameters, featurize and reward functions are used to create an environment consisting of the electircal power grid. To keep the first learning example simple the action given to the env is internally not delayed. </p><pre><code class="language-julia hljs">env = ElectricGridEnv(
    CM = CM, 
    parameters = parameters, 
    t_end = 0.1, 
    featurize = featurize_ddpg, 
    reward_function = reward_function, 
    action_delay = 0)</code></pre><p>In this example a <code>Deep Deterministic Policy Gradient</code> agent (https://arxiv.org/abs/1509.02971, https://spinningup.openai.com/en/latest/algorithms/ddpg.html) is chosen which can learn a control task on continous state and action spaces. It is configured using the <code>CreateAgentDdpg()</code> function which uses the information about the state and action ids, based on the parameter dict, stored in the <code>agent_dict</code> in the env:</p><p><code>env.agent_dict[chosen_key]</code> (chosen key, here, <code>my_ddpg</code>):</p><ul><li><code>&quot;source_number&quot;</code>: ID/number of the source the agent with this key controls</li><li><code>&quot;mode&quot;</code>: Name of the agent</li><li><code>&quot;action_ids&quot;</code>: List of strings with the action ids the agent controls/belong to the &quot;source_number&quot;`</li><li><code>&quot;state_ids&quot;</code>: List of strings with the state ids the agent controls/belong to the &quot;source_number&quot;`</li></ul><p>This information is used in the <code>SetupAgents()</code> method to configure the control-side of the experiment.</p><p>The agent is configured to receive as many inputs as environment returns for it&#39;s state (after <code>featurize</code>) and return as many outputs as actions requested from the env corresponding to the ids.</p><pre><code class="language-julia hljs">agent = CreateAgentDdpg(na = length(env.agent_dict[&quot;my_ddpg&quot;][&quot;action_ids&quot;]),
                          ns = length(state(env, &quot;my_ddpg&quot;)),
                          use_gpu = false)</code></pre><p>The <code>SetupAgents()</code> function takes the control types defined in the parameter dict and hands the correct indices to the corrensponding controllers / agents. The function returns <code>controllers</code> which is an instance of the <code>MultiController</code> which contains the different agents and classic controllers and maps their actions to the corresponding sources. </p><p>Since in this example only one RL agent will be used it only contains the defined <code>my_ddpg</code> agent.  Therefore, the agent handed over to the <code>SetupAgents()</code> function is internally extended by a name to a <code>named policy</code> (https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.NamedPolicy ). Using this name the <code>MultiController</code> (compare, https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.MADDPGManager) enables to call the different agents/controllers via name during training and application.</p><p>To use the previously defined agent, a dict linking tha <code>chosen_key</code>: <code>my_ddpg</code> to the defined RL agent is handed over to the <code>SetupAgents</code> method: </p><pre><code class="language-julia hljs">my_custom_agents = Dict(&quot;my_ddpg&quot; =&gt; agent)

controllers = SetupAgents(env, my_custom_agents)</code></pre><p>The <code>controllers</code> in this examples consits only of the one RL agent (<code>my_ddpg</code>) and can be trained usin the <code>Learn()</code> function to train 20 episodes:</p><pre><code class="language-julia hljs">Learn(controllers, env, num_episodes = 20)</code></pre><p>After the training, the <code>Simulate()</code> function is used to run a test epiode without action noise and the state to be controlled (<span>$i_\mathrm{L1}$</span>) is plotted:</p><pre><code class="language-julia hljs">
states_to_plot = [&quot;source1_i_L1&quot;]
hook = DataHook(collect_state_ids = states_to_plot)

Simulate(controllers, env, hook=hook)

RenderHookResults(hook = hook,
                  states_to_plot  = states_to_plot)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../classical/">« Set up Classical Controllers</a><a class="docs-footer-nextpage" href="../nodeconstructor/">The Nodeconstructor »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Wednesday 22 March 2023 16:49">Wednesday 22 March 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
